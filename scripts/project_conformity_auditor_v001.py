#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ” AUDITEUR CONFORMITÃ‰ PROJET
====================================================================
VÃ©rification complÃ¨te de la conformitÃ© du projet PaniniFS-Research
aux rÃ¨gles de copilotage consolidÃ©es.

Auteur: Assistant IA PaniniFS Research
Version: 0.0.1 - Audit ConformitÃ© Projet
Date: 08/09/2025
"""

import os
import json
import re
from pathlib import Path
from typing import Dict, List, Tuple
from dataclasses import dataclass
from datetime import datetime

@dataclass
class ConformityIssue:
    """Issue de conformitÃ© identifiÃ©e"""
    file_path: str
    issue_type: str
    severity: str  # "critical", "major", "minor"
    description: str
    expected: str
    current: str
    fix_suggestion: str

@dataclass
class DirectoryAnalysis:
    """Analyse d'un rÃ©pertoire"""
    path: str
    expected_content: List[str]
    actual_content: List[str]
    missing_files: List[str]
    unexpected_files: List[str]
    naming_issues: List[str]

class ProjectConformityAuditor:
    """Auditeur conformitÃ© projet"""
    
    def __init__(self):
        print("ğŸ” AUDIT CONFORMITÃ‰ PROJET PANINIFIX-RESEARCH")
        
        self.project_root = Path("/home/stephane/GitHub/PaniniFS-Research")
        self.issues = []
        
        # RÃ¨gles de nommage selon REGLES_COPILOTAGE_v0.0.1.md
        self.naming_rules = {
            "rapport": r"^RAPPORT_[A-Z_]+_v\d+\.\d+\.\d+\.md$",
            "analyse": r"^ANALYSE_[A-Z_]+[A-Z0-9_]*\.md$",
            "cache": r"^CACHE_[A-Z_]+_[A-Z0-9_]*\.json$",
            "validation": r"^VALIDATION_[A-Z_]+[A-Z0-9_]*\.md$",
            "recherche": r"^RECHERCHE_[A-Z_]+_v\d+\.\d+\.\d+\.md$",
            "tableau": r"^TABLEAU_[A-Z_]+_v\d+\.\d+\.\d+\.(md|csv)$",
            "regles": r"^REGLES_[A-Z_]+_v\d+\.\d+\.\d+\.md$",
            "verification": r"^VERIFICATION_[A-Z_]+_v\d+\.\d+\.\d+\.md$"
        }
        
        # Structure attendue
        self.expected_structure = {
            "data/corpus_babillage": ["Corpus d'analyse linguistique"],
            "data/references_cache": ["Cache rÃ©fÃ©rences + rapports"],
            "scripts": ["Scripts Python d'analyse"],
            "discoveries": ["DÃ©couvertes recherche"],
            "publications": ["Articles et livres"],
            "methodology": ["Protocoles mÃ©thodologiques"],
            "docs": ["Documentation projet"]
        }
    
    def check_directory_structure(self) -> Dict:
        """VÃ©rification structure rÃ©pertoires"""
        print("\nğŸ“ VÃ‰RIFICATION STRUCTURE RÃ‰PERTOIRES")
        
        structure_issues = []
        
        for expected_dir, description in self.expected_structure.items():
            dir_path = self.project_root / expected_dir
            
            if not dir_path.exists():
                structure_issues.append(ConformityIssue(
                    file_path=str(dir_path),
                    issue_type="missing_directory",
                    severity="major",
                    description=f"RÃ©pertoire manquant: {expected_dir}",
                    expected=f"RÃ©pertoire {expected_dir} doit exister",
                    current="RÃ©pertoire inexistant",
                    fix_suggestion=f"CrÃ©er: mkdir -p {dir_path}"
                ))
            else:
                print(f"   âœ… {expected_dir} - {description[0]}")
        
        return {
            "issues": structure_issues,
            "directories_checked": len(self.expected_structure),
            "missing_directories": len(structure_issues)
        }
    
    def check_file_naming_conventions(self) -> Dict:
        """VÃ©rification conventions nommage"""
        print("\nğŸ“ VÃ‰RIFICATION CONVENTIONS NOMMAGE")
        
        naming_issues = []
        files_checked = 0
        
        # VÃ©rification data/references_cache
        cache_dir = self.project_root / "data/references_cache"
        if cache_dir.exists():
            for file_path in cache_dir.iterdir():
                if file_path.is_file():
                    files_checked += 1
                    filename = file_path.name
                    
                    # DÃ©terminer type attendu
                    file_type = self._determine_file_type(filename)
                    
                    if file_type and file_type in self.naming_rules:
                        pattern = self.naming_rules[file_type]
                        if not re.match(pattern, filename):
                            naming_issues.append(ConformityIssue(
                                file_path=str(file_path),
                                issue_type="naming_convention",
                                severity="minor",
                                description=f"Nom fichier non-conforme: {filename}",
                                expected=f"Pattern: {pattern}",
                                current=filename,
                                fix_suggestion=f"Renommer selon convention {file_type}"
                            ))
                        else:
                            print(f"   âœ… {filename} - conforme")
                    elif filename not in ["metadata.json", "references_cache.json"]:
                        naming_issues.append(ConformityIssue(
                            file_path=str(file_path),
                            issue_type="unknown_file_type",
                            severity="minor",
                            description=f"Type fichier non-identifiÃ©: {filename}",
                            expected="Type fichier reconnu",
                            current="Type inconnu",
                            fix_suggestion="VÃ©rifier si fichier nÃ©cessaire ou renommer"
                        ))
        
        # VÃ©rification scripts
        scripts_dir = self.project_root / "scripts"
        if scripts_dir.exists():
            for file_path in scripts_dir.iterdir():
                if file_path.suffix == ".py":
                    files_checked += 1
                    filename = file_path.name
                    
                    # Convention scripts: [nom]_v[version].py
                    script_pattern = r"^[a-z_]+_v\d{3}\.py$"
                    if not re.match(script_pattern, filename):
                        naming_issues.append(ConformityIssue(
                            file_path=str(file_path),
                            issue_type="script_naming",
                            severity="minor",
                            description=f"Script mal nommÃ©: {filename}",
                            expected="Pattern: [nom]_v[version].py",
                            current=filename,
                            fix_suggestion="Renommer avec version 3 chiffres"
                        ))
                    else:
                        print(f"   âœ… {filename} - script conforme")
        
        return {
            "issues": naming_issues,
            "files_checked": files_checked,
            "naming_violations": len(naming_issues)
        }
    
    def _determine_file_type(self, filename: str) -> str:
        """DÃ©termine le type d'un fichier"""
        filename_upper = filename.upper()
        
        if filename_upper.startswith("RAPPORT_"):
            return "rapport"
        elif filename_upper.startswith("ANALYSE_"):
            return "analyse"  
        elif filename_upper.startswith("CACHE_"):
            return "cache"
        elif filename_upper.startswith("VALIDATION_"):
            return "validation"
        elif filename_upper.startswith("RECHERCHE_"):
            return "recherche"
        elif filename_upper.startswith("TABLEAU_"):
            return "tableau"
        elif filename_upper.startswith("REGLES_"):
            return "regles"
        elif filename_upper.startswith("VERIFICATION_"):
            return "verification"
        
        return None
    
    def check_metadata_completeness(self) -> Dict:
        """VÃ©rification complÃ©tude mÃ©tadonnÃ©es"""
        print("\nğŸ“Š VÃ‰RIFICATION MÃ‰TADONNÃ‰ES")
        
        metadata_issues = []
        
        # VÃ©rification metadata.json principal
        metadata_path = self.project_root / "data/references_cache/metadata.json"
        if metadata_path.exists():
            try:
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                required_fields = [
                    "cache_metadata.created",
                    "cache_metadata.last_updated", 
                    "cache_metadata.version",
                    "cache_metadata.purpose"
                ]
                
                for field in required_fields:
                    if not self._check_nested_field(metadata, field):
                        metadata_issues.append(ConformityIssue(
                            file_path=str(metadata_path),
                            issue_type="missing_metadata_field",
                            severity="major",
                            description=f"Champ manquant: {field}",
                            expected=f"Champ {field} prÃ©sent",
                            current="Champ absent",
                            fix_suggestion=f"Ajouter champ {field}"
                        ))
                
                print(f"   âœ… metadata.json - {len(required_fields) - len(metadata_issues)} champs sur {len(required_fields)}")
                
            except json.JSONDecodeError:
                metadata_issues.append(ConformityIssue(
                    file_path=str(metadata_path),
                    issue_type="invalid_json",
                    severity="critical",
                    description="Fichier JSON invalide",
                    expected="JSON valide",
                    current="JSON corrompu",
                    fix_suggestion="Corriger syntaxe JSON"
                ))
        else:
            metadata_issues.append(ConformityIssue(
                file_path=str(metadata_path),
                issue_type="missing_file",
                severity="critical",
                description="Fichier metadata.json manquant",
                expected="Fichier metadata.json prÃ©sent",
                current="Fichier absent",
                fix_suggestion="CrÃ©er metadata.json avec structure requise"
            ))
        
        # VÃ©rification references_cache.json
        ref_cache_path = self.project_root / "data/references_cache/references_cache.json"
        if ref_cache_path.exists():
            try:
                with open(ref_cache_path, 'r', encoding='utf-8') as f:
                    ref_cache = json.load(f)
                
                if "groups" not in ref_cache:
                    metadata_issues.append(ConformityIssue(
                        file_path=str(ref_cache_path),
                        issue_type="missing_cache_structure",
                        severity="major", 
                        description="Structure cache rÃ©fÃ©rences incomplÃ¨te",
                        expected="Champ 'groups' prÃ©sent",
                        current="Structure manquante",
                        fix_suggestion="Ajouter structure complÃ¨te cache"
                    ))
                else:
                    print(f"   âœ… references_cache.json - structure valide")
                    
            except json.JSONDecodeError:
                metadata_issues.append(ConformityIssue(
                    file_path=str(ref_cache_path),
                    issue_type="invalid_json",
                    severity="critical",
                    description="Cache rÃ©fÃ©rences JSON invalide",
                    expected="JSON valide",
                    current="JSON corrompu", 
                    fix_suggestion="Corriger syntaxe JSON cache"
                ))
        else:
            metadata_issues.append(ConformityIssue(
                file_path=str(ref_cache_path),
                issue_type="missing_file",
                severity="major",
                description="Cache rÃ©fÃ©rences manquant",
                expected="Fichier references_cache.json prÃ©sent",
                current="Fichier absent",
                fix_suggestion="CrÃ©er cache rÃ©fÃ©rences"
            ))
        
        return {
            "issues": metadata_issues,
            "files_checked": 2,
            "metadata_violations": len(metadata_issues)
        }
    
    def _check_nested_field(self, data: dict, field_path: str) -> bool:
        """VÃ©rification champ imbriquÃ©"""
        keys = field_path.split('.')
        current = data
        
        for key in keys:
            if isinstance(current, dict) and key in current:
                current = current[key]
            else:
                return False
        
        return True
    
    def check_documentation_completeness(self) -> Dict:
        """VÃ©rification complÃ©tude documentation"""
        print("\nğŸ“š VÃ‰RIFICATION DOCUMENTATION")
        
        doc_issues = []
        
        # VÃ©rification README principal
        readme_path = self.project_root / "README.md"
        if not readme_path.exists():
            doc_issues.append(ConformityIssue(
                file_path=str(readme_path),
                issue_type="missing_documentation",
                severity="major",
                description="README.md principal manquant",
                expected="README.md Ã  la racine",
                current="Fichier absent",
                fix_suggestion="CrÃ©er README.md principal"
            ))
        else:
            print("   âœ… README.md principal prÃ©sent")
        
        # VÃ©rification rÃ¨gles copilotage
        rules_path = self.project_root / "data/references_cache/REGLES_COPILOTAGE_v0.0.1.md"
        if rules_path.exists():
            print("   âœ… RÃ¨gles copilotage prÃ©sentes")
        else:
            doc_issues.append(ConformityIssue(
                file_path=str(rules_path),
                issue_type="missing_rules",
                severity="major",
                description="RÃ¨gles copilotage manquantes",
                expected="REGLES_COPILOTAGE_v0.0.1.md",
                current="Fichier absent",
                fix_suggestion="CrÃ©er rÃ¨gles copilotage"
            ))
        
        # VÃ©rification docs/README.md
        docs_readme = self.project_root / "docs/README.md"
        if docs_readme.exists():
            print("   âœ… docs/README.md prÃ©sent")
        else:
            doc_issues.append(ConformityIssue(
                file_path=str(docs_readme),
                issue_type="missing_docs_readme",
                severity="minor",
                description="Documentation docs/ incomplÃ¨te",
                expected="docs/README.md",
                current="Fichier absent",
                fix_suggestion="CrÃ©er documentation docs/"
            ))
        
        return {
            "issues": doc_issues,
            "files_checked": 3,
            "documentation_violations": len(doc_issues)
        }
    
    def generate_conformity_report(self) -> str:
        """GÃ©nÃ©ration rapport conformitÃ© complet"""
        print("\nğŸ“‹ GÃ‰NÃ‰RATION RAPPORT CONFORMITÃ‰")
        
        # ExÃ©cution tous les audits
        structure_audit = self.check_directory_structure()
        naming_audit = self.check_file_naming_conventions()
        metadata_audit = self.check_metadata_completeness()
        doc_audit = self.check_documentation_completeness()
        
        # Consolidation issues
        all_issues = (structure_audit["issues"] + 
                     naming_audit["issues"] + 
                     metadata_audit["issues"] + 
                     doc_audit["issues"])
        
        # Classification par sÃ©vÃ©ritÃ©
        critical_issues = [i for i in all_issues if i.severity == "critical"]
        major_issues = [i for i in all_issues if i.severity == "major"]
        minor_issues = [i for i in all_issues if i.severity == "minor"]
        
        # Calcul score conformitÃ©
        total_checks = (structure_audit["directories_checked"] + 
                       naming_audit["files_checked"] + 
                       metadata_audit["files_checked"] + 
                       doc_audit["files_checked"])
        
        total_violations = len(all_issues)
        conformity_score = max(0, 100 - (total_violations * 5))  # -5 points par violation
        
        # GÃ©nÃ©ration rapport
        report_path = self.project_root / "data/references_cache/AUDIT_CONFORMITE_v0.0.1.md"
        
        report_content = f"""# ğŸ” AUDIT CONFORMITÃ‰ PROJET v0.0.1

## ğŸ¯ **RÃ©sultats Audit ConformitÃ© PaniniFS-Research**

### **ğŸ“Š Score Global de ConformitÃ©: {conformity_score:.1f}/100**

- **Total vÃ©rifications**: {total_checks}
- **Violations identifiÃ©es**: {total_violations}
- **Issues critiques**: {len(critical_issues)}
- **Issues majeures**: {len(major_issues)}
- **Issues mineures**: {len(minor_issues)}

### **ğŸ† Ã‰tat ConformitÃ© par CatÃ©gorie**

#### **ğŸ“ Structure RÃ©pertoires**
- RÃ©pertoires vÃ©rifiÃ©s: {structure_audit["directories_checked"]}
- RÃ©pertoires manquants: {structure_audit["missing_directories"]}
- Statut: {"âœ… CONFORME" if structure_audit["missing_directories"] == 0 else f"âŒ {structure_audit['missing_directories']} manquants"}

#### **ğŸ“ Conventions Nommage**
- Fichiers vÃ©rifiÃ©s: {naming_audit["files_checked"]}
- Violations nommage: {naming_audit["naming_violations"]}
- Statut: {"âœ… CONFORME" if naming_audit["naming_violations"] == 0 else f"âš ï¸ {naming_audit['naming_violations']} violations"}

#### **ğŸ“Š MÃ©tadonnÃ©es**
- Fichiers vÃ©rifiÃ©s: {metadata_audit["files_checked"]}
- Violations mÃ©tadonnÃ©es: {metadata_audit["metadata_violations"]}
- Statut: {"âœ… CONFORME" if metadata_audit["metadata_violations"] == 0 else f"âŒ {metadata_audit['metadata_violations']} violations"}

#### **ğŸ“š Documentation**
- Fichiers vÃ©rifiÃ©s: {doc_audit["files_checked"]}
- Violations documentation: {doc_audit["documentation_violations"]}
- Statut: {"âœ… CONFORME" if doc_audit["documentation_violations"] == 0 else f"âš ï¸ {doc_audit['documentation_violations']} violations"}

## ğŸš¨ **Issues Critiques Ã  Corriger ImmÃ©diatement**

{chr(10).join(f'''
### **{i+1}. {issue.issue_type.replace('_', ' ').title()}**
- **Fichier**: `{issue.file_path}`
- **Description**: {issue.description}
- **Attendu**: {issue.expected}
- **Actuel**: {issue.current}
- **Solution**: {issue.fix_suggestion}
''' for i, issue in enumerate(critical_issues)) if critical_issues else "ğŸ‰ **Aucune issue critique identifiÃ©e !**"}

## âš ï¸ **Issues Majeures Ã  Traiter**

{chr(10).join(f'''
### **{i+1}. {issue.issue_type.replace('_', ' ').title()}**
- **Fichier**: `{issue.file_path}`
- **Description**: {issue.description}
- **Solution**: {issue.fix_suggestion}
''' for i, issue in enumerate(major_issues)) if major_issues else "âœ… **Aucune issue majeure identifiÃ©e !**"}

## ğŸ’¡ **Issues Mineures (AmÃ©liorations)**

{chr(10).join(f'''
### **{i+1}. {issue.issue_type.replace('_', ' ').title()}**
- **Fichier**: `{issue.file_path}`
- **Description**: {issue.description}
- **Solution**: {issue.fix_suggestion}
''' for i, issue in enumerate(minor_issues[:5])) if minor_issues else "ğŸ¯ **Aucune amÃ©lioration mineure identifiÃ©e !**"}

{f"... et {len(minor_issues) - 5} autres issues mineures" if len(minor_issues) > 5 else ""}

## ğŸ”§ **Plan d'Action RecommandÃ©**

### **ğŸš¨ PrioritÃ© 1 (Issues Critiques)**
{chr(10).join(f"1. {issue.fix_suggestion}" for issue in critical_issues[:3]) if critical_issues else "âœ… Aucune action critique requise"}

### **âš ï¸ PrioritÃ© 2 (Issues Majeures)**
{chr(10).join(f"1. {issue.fix_suggestion}" for issue in major_issues[:3]) if major_issues else "âœ… Aucune action majeure requise"}

### **ğŸ’¡ PrioritÃ© 3 (AmÃ©liorations)**
{chr(10).join(f"1. {issue.fix_suggestion}" for issue in minor_issues[:3]) if minor_issues else "âœ… Aucune amÃ©lioration requise"}

## ğŸ“ˆ **Recommandations Maintien ConformitÃ©**

### **Routine Quotidienne**
- VÃ©rifier nommage nouveaux fichiers
- Mettre Ã  jour mÃ©tadonnÃ©es aprÃ¨s modifications
- Documenter nouvelles analyses

### **Routine Hebdomadaire**
- Audit conformitÃ© automatique
- Nettoyage fichiers obsolÃ¨tes
- Consolidation cache rÃ©fÃ©rences

### **Routine Mensuelle**
- RÃ©vision rÃ¨gles copilotage
- Audit complet structure projet
- Mise Ã  jour documentation

## âœ… **Validation Audit**

**Audit rÃ©alisÃ© le**: {datetime.now().strftime('%d/%m/%Y Ã  %H:%M')}
**ConformitÃ© aux rÃ¨gles**: REGLES_COPILOTAGE_v0.0.1.md
**Prochaine rÃ©vision**: RecommandÃ©e dans 7 jours

### **CritÃ¨res Validation**
- âœ… Structure rÃ©pertoires vÃ©rifiÃ©e
- âœ… Conventions nommage auditÃ©es  
- âœ… MÃ©tadonnÃ©es validÃ©es
- âœ… Documentation Ã©valuÃ©e
- âœ… Plan d'action Ã©tabli

---

**Audit ConformitÃ© v0.0.1 TERMINÃ‰** âœ“  
*Maintien qualitÃ© projet validÃ©*

---
*GÃ©nÃ©rÃ© le {datetime.now().strftime('%d/%m/%Y Ã  %H:%M')}*
"""
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        return str(report_path)

def run_project_audit():
    """ExÃ©cution audit complet projet"""
    print("ğŸ” AUDIT CONFORMITÃ‰ PROJET PANINIFIX-RESEARCH")
    print("=" * 60)
    
    auditor = ProjectConformityAuditor()
    
    # GÃ©nÃ©ration rapport complet
    report_path = auditor.generate_conformity_report()
    
    print(f"\nğŸ“„ Rapport audit: {report_path}")
    print(f"\nğŸ¯ Audit terminÃ© - Consultez le rapport pour dÃ©tails")
    
    print("\nâœ… AUDIT CONFORMITÃ‰ TERMINÃ‰!")

if __name__ == "__main__":
    run_project_audit()
