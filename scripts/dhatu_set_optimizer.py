#!/usr/bin/env python3
"""
Optimiseur de Set Minimal de DhÄtu pour PaniniFS
Recherche le nombre optimal de dhÄtu pour couverture sÃ©mantique maximale
"""

import json
import itertools
from typing import Dict, List, Set, Tuple
from dataclasses import dataclass
from collections import defaultdict
from semantic_coverage_analyzer import SemanticCoverageAnalyzer
from dhatu_candidate_generator import DhatuCandidateGenerator, DhatuCandidate

@dataclass
class OptimizationResult:
    """RÃ©sultat d'une optimisation de set dhÄtu"""
    dhatu_set: List[str]
    coverage_score: float
    efficiency_ratio: float  # coverage / dhatu_count
    semantic_completeness: float  # % concepts sÃ©mantiques couverts
    redundancy_score: float  # Niveau de redondance entre dhÄtu

class DhatuSetOptimizer:
    """Optimiseur pour trouver le set minimal de dhÄtu optimal"""
    
    def __init__(self):
        self.analyzer = SemanticCoverageAnalyzer()
        self.generator = DhatuCandidateGenerator()
        
        # DÃ©finition des nouveaux dhÄtu proposÃ©s avec leurs patterns
        self.extended_dhatu_patterns = {
            # DhÄtu originaux
            **self.analyzer.dhatu_patterns,
            
            # Nouveaux dhÄtu candidats
            'EXIST': {
                'patterns': [
                    r'\b(exists?|there is|there are|being|reality|presence)\b',
                    r'\b(existe|il y a|Ãªtre|rÃ©alitÃ©|prÃ©sence)\b',
                    r'\b(available|present|absent|missing)\b'
                ],
                'concepts': ['existence', 'presence', 'being', 'availability']
            },
            'EVAL': {
                'patterns': [
                    r'\b(elegant|beautiful|ugly|good|bad|better|worse|quality)\b',
                    r'\b(Ã©lÃ©gant|beau|laid|bon|mauvais|meilleur|pire|qualitÃ©)\b',
                    r'\b(clean|robust|efficient|optimal|superior)\b'
                ],
                'concepts': ['evaluation', 'quality', 'assessment', 'aesthetics']
            },
            'CAUSE': {
                'patterns': [
                    r'\b(because|since|due to|causes?|results in|leads to)\b',
                    r'\b(parce que|car|Ã  cause de|provoque|entraÃ®ne|mÃ¨ne Ã )\b',
                    r'\b(therefore|thus|consequently|hence)\b'
                ],
                'concepts': ['causation', 'consequence', 'reason', 'implication']
            },
            'MODAL': {
                'patterns': [
                    r'\b(might|could|may|possible|potential|maybe|perhaps)\b',
                    r'\b(pourrait|peut-Ãªtre|possible|potentiel|Ã©ventuellement)\b',
                    r'\b(probably|likely|unlikely|chance)\b'
                ],
                'concepts': ['possibility', 'probability', 'modality', 'uncertainty']
            },
            'RELATE': {
                'patterns': [
                    r'\b(relates to|in relation to|connected to|linked to|associated)\b',
                    r'\b(en relation avec|liÃ© Ã |associÃ© Ã |connectÃ© Ã )\b',
                    r'\b(compared to|versus|relative to|with respect to)\b'
                ],
                'concepts': ['relation', 'connection', 'association', 'comparison']
            },
            'FEEL': {
                'patterns': [
                    r'\b(love|hate|like|prefer|enjoy|appreciate|dislike)\b',
                    r'\b(aime|dÃ©teste|prÃ©fÃ¨re|apprÃ©cie|n\'aime pas)\b',
                    r'\b(satisfying|frustrating|exciting|boring|pleasant)\b'
                ],
                'concepts': ['emotion', 'preference', 'feeling', 'sentiment']
            },
            'FLOW': {
                'patterns': [
                    r'\b(then|after|before|next|previous|timeline|sequence)\b',
                    r'\b(puis|aprÃ¨s|avant|suivant|prÃ©cÃ©dent|chronologie)\b',
                    r'\b(meanwhile|simultaneously|eventually|finally)\b'
                ],
                'concepts': ['temporal_flow', 'progression', 'chronology', 'timing']
            }
        }
    
    def find_minimal_optimal_set(self, target_coverage: float = 90.0, 
                                max_dhatu: int = 12, 
                                test_corpus: List[str] = None) -> Dict:
        """Trouve le set minimal de dhÄtu pour atteindre la couverture cible"""
        
        if test_corpus is None:
            test_corpus = self._get_default_test_corpus()
        
        all_dhatu = list(self.extended_dhatu_patterns.keys())
        best_result = None
        results_by_size = defaultdict(list)
        
        print(f"ğŸ” Recherche du set optimal (cible: {target_coverage}%, max: {max_dhatu} dhÄtu)")
        print(f"ğŸ“Š Corpus de test: {len(test_corpus)} textes")
        print(f"ğŸ§¬ DhÄtu disponibles: {len(all_dhatu)} ({', '.join(all_dhatu)})")
        
        # Tester des sets de taille croissante
        for size in range(7, min(max_dhatu + 1, len(all_dhatu) + 1)):
            print(f"\nâš™ï¸  Test des combinaisons de {size} dhÄtu...")
            
            # Limiter le nombre de combinaisons testÃ©es pour Ã©viter l'explosion combinatoire
            max_combinations = 1000 if size <= 10 else 500
            combinations_tested = 0
            
            for dhatu_combination in itertools.combinations(all_dhatu, size):
                if combinations_tested >= max_combinations:
                    print(f"   (Limite de {max_combinations} combinaisons atteinte)")
                    break
                
                # Tester cette combinaison
                result = self._evaluate_dhatu_set(list(dhatu_combination), test_corpus)
                results_by_size[size].append(result)
                
                # VÃ©rifier si c'est le meilleur rÃ©sultat jusqu'Ã  prÃ©sent
                if (best_result is None or 
                    (result.coverage_score >= target_coverage and 
                     result.efficiency_ratio > best_result.efficiency_ratio) or
                    (best_result.coverage_score < target_coverage and 
                     result.coverage_score > best_result.coverage_score)):
                    best_result = result
                
                combinations_tested += 1
                
                # Affichage de progression
                if combinations_tested % 100 == 0:
                    print(f"   {combinations_tested} combinaisons testÃ©es...")
            
            # Si on a atteint la cible avec cette taille, on peut s'arrÃªter
            best_for_size = max(results_by_size[size], key=lambda x: x.coverage_score)
            print(f"   Meilleur pour {size} dhÄtu: {best_for_size.coverage_score:.1f}% "
                  f"(efficacitÃ©: {best_for_size.efficiency_ratio:.1f}%)")
            
            if best_for_size.coverage_score >= target_coverage:
                print(f"   âœ… Cible atteinte avec {size} dhÄtu !")
                break
        
        return {
            'optimal_result': best_result,
            'results_by_size': dict(results_by_size),
            'analysis': self._analyze_optimization_results(results_by_size, target_coverage),
            'recommendations': self._generate_optimization_recommendations(best_result, target_coverage)
        }
    
    def _evaluate_dhatu_set(self, dhatu_set: List[str], test_corpus: List[str]) -> OptimizationResult:
        """Ã‰value un set specific de dhÄtu sur le corpus de test"""
        
        # CrÃ©er un analyseur temporaire avec ce set de dhÄtu
        temp_patterns = {k: v for k, v in self.extended_dhatu_patterns.items() if k in dhatu_set}
        temp_analyzer = SemanticCoverageAnalyzer()
        temp_analyzer.dhatu_patterns = temp_patterns
        
        total_coverage = []
        semantic_concepts_covered = set()
        semantic_concepts_total = set()
        
        for text in test_corpus:
            result = temp_analyzer.analyze_text(text)
            total_coverage.append(result['coverage_score']['percentage'])
            
            # Compter les concepts sÃ©mantiques
            for match in result['dhatu_matches']:
                semantic_concepts_covered.add(match.concept)
            
            # Estimer les concepts totaux (y compris gaps)
            for gap in result['semantic_gaps']:
                semantic_concepts_total.add(gap.semantic_category)
        
        avg_coverage = sum(total_coverage) / len(total_coverage) if total_coverage else 0
        semantic_completeness = len(semantic_concepts_covered) / max(len(semantic_concepts_covered) + len(semantic_concepts_total), 1) * 100
        efficiency_ratio = avg_coverage / len(dhatu_set) if dhatu_set else 0
        
        # Calculer la redondance (concepts qui se chevauchent)
        redundancy_score = self._calculate_redundancy(dhatu_set, temp_patterns)
        
        return OptimizationResult(
            dhatu_set=dhatu_set,
            coverage_score=avg_coverage,
            efficiency_ratio=efficiency_ratio,
            semantic_completeness=semantic_completeness,
            redundancy_score=redundancy_score
        )
    
    def _calculate_redundancy(self, dhatu_set: List[str], patterns: Dict) -> float:
        """Calcule le score de redondance entre dhÄtu (patterns qui se chevauchent)"""
        
        # Analyse simple : compter les mots-clÃ©s partagÃ©s entre patterns
        all_words = defaultdict(list)
        
        for dhatu_name in dhatu_set:
            if dhatu_name in patterns:
                for pattern in patterns[dhatu_name]['patterns']:
                    # Extraire les mots des patterns regex
                    words = pattern.replace(r'\b', '').replace('(', '').replace(')', '').split('|')
                    for word in words:
                        if word.strip():
                            all_words[word.strip().lower()].append(dhatu_name)
        
        # Calculer le pourcentage de mots partagÃ©s
        shared_words = sum(1 for dhatu_list in all_words.values() if len(dhatu_list) > 1)
        total_words = len(all_words)
        
        return (shared_words / total_words * 100) if total_words > 0 else 0
    
    def _get_default_test_corpus(self) -> List[str]:
        """Corpus de test par dÃ©faut pour l'optimisation"""
        return [
            "I love this elegant solution because it efficiently transforms data",
            "Cette approche est robuste car elle gÃ¨re bien les cas d'erreur", 
            "for i in range(10): print(i) if i > 5",
            "The algorithm exists to solve the problem, but it might sometimes fail",
            "En relation avec notre discussion, ce code est plus lisible",
            "Cette fonction est belle et efficace comparÃ©e Ã  l'ancienne version",
            "Le systÃ¨me pourrait Ãªtre amÃ©liorÃ© si nous ajoutons cette fonctionnalitÃ©",
            "Par opposition aux autres solutions, celle-ci est plus maintenable",
            "Il y a plusieurs faÃ§ons d'implÃ©menter cela, mais cette approche est prÃ©fÃ©rable",
            "Pendant que le processus s'exÃ©cute, nous pouvons faire autre chose",
            "Cette mÃ©thode fonctionne bien parce qu'elle suit les bonnes pratiques",
            "def process_data(input_list): return [x*2 for x in input_list if x > 0]",
            "The user interface feels intuitive and responds quickly to input",
            "We need to locate the bug and decide how to fix it systematically",
            "This pattern groups similar items and sequences them logically"
        ]
    
    def _analyze_optimization_results(self, results_by_size: Dict, target_coverage: float) -> Dict:
        """Analyse les rÃ©sultats d'optimisation pour identifier les patterns"""
        
        analysis = {
            'coverage_progression': {},
            'efficiency_sweet_spot': None,
            'minimal_viable_size': None,
            'optimal_size': None
        }
        
        for size, results in results_by_size.items():
            best_for_size = max(results, key=lambda x: x.coverage_score)
            analysis['coverage_progression'][size] = {
                'best_coverage': best_for_size.coverage_score,
                'best_efficiency': best_for_size.efficiency_ratio,
                'avg_coverage': sum(r.coverage_score for r in results) / len(results)
            }
            
            # Identifier la taille minimale viable (atteint la cible)
            if (analysis['minimal_viable_size'] is None and 
                best_for_size.coverage_score >= target_coverage):
                analysis['minimal_viable_size'] = size
            
            # Identifier le sweet spot d'efficacitÃ©
            if (analysis['efficiency_sweet_spot'] is None or 
                best_for_size.efficiency_ratio > analysis['efficiency_sweet_spot'][1]):
                analysis['efficiency_sweet_spot'] = (size, best_for_size.efficiency_ratio)
        
        # Identifier la taille optimale (Ã©quilibre couverture/efficacitÃ©)
        if analysis['minimal_viable_size']:
            analysis['optimal_size'] = analysis['minimal_viable_size']
        else:
            # Prendre la taille avec la meilleure efficacitÃ©
            if analysis['efficiency_sweet_spot']:
                analysis['optimal_size'] = analysis['efficiency_sweet_spot'][0]
        
        return analysis
    
    def _generate_optimization_recommendations(self, best_result: OptimizationResult, 
                                             target_coverage: float) -> List[str]:
        """GÃ©nÃ¨re des recommandations basÃ©es sur les rÃ©sultats d'optimisation"""
        
        recommendations = []
        
        if best_result is None:
            return ["Aucun rÃ©sultat d'optimisation disponible"]
        
        if best_result.coverage_score >= target_coverage:
            recommendations.append(f"âœ… Cible atteinte avec {len(best_result.dhatu_set)} dhÄtu "
                                 f"({best_result.coverage_score:.1f}%)")
        else:
            recommendations.append(f"âš ï¸  Cible non atteinte : {best_result.coverage_score:.1f}% "
                                 f"vs {target_coverage}% visÃ©")
        
        if best_result.efficiency_ratio > 8.0:
            recommendations.append("ğŸ¯ Excellente efficacitÃ© par dhÄtu")
        elif best_result.efficiency_ratio > 6.0:
            recommendations.append("ğŸ‘ Bonne efficacitÃ© par dhÄtu")
        else:
            recommendations.append("âš ï¸  EfficacitÃ© faible - considÃ©rer rÃ©duction du set")
        
        if best_result.redundancy_score > 20:
            recommendations.append("ğŸ”„ Redondance Ã©levÃ©e dÃ©tectÃ©e - optimisation possible")
        
        if best_result.semantic_completeness > 80:
            recommendations.append("ğŸŒ Excellente couverture des concepts sÃ©mantiques")
        
        return recommendations

def main():
    """Fonction de test et dÃ©monstration"""
    optimizer = DhatuSetOptimizer()
    
    print("âš¡ OPTIMISEUR DE SET MINIMAL DHÄ€TU")
    print("=" * 45)
    
    # Test avec diffÃ©rents objectifs de couverture
    targets = [80.0, 85.0, 90.0]
    
    for target in targets:
        print(f"\nğŸ¯ OPTIMISATION POUR {target}% DE COUVERTURE")
        print("-" * 40)
        
        result = optimizer.find_minimal_optimal_set(
            target_coverage=target,
            max_dhatu=12
        )
        
        if result['optimal_result']:
            opt = result['optimal_result']
            print(f"Set optimal: {', '.join(opt.dhatu_set)}")
            print(f"Taille: {len(opt.dhatu_set)} dhÄtu")
            print(f"Couverture: {opt.coverage_score:.1f}%")
            print(f"EfficacitÃ©: {opt.efficiency_ratio:.1f}% par dhÄtu")
            print(f"ComplÃ©tude sÃ©mantique: {opt.semantic_completeness:.1f}%")
            print(f"Redondance: {opt.redundancy_score:.1f}%")
            
            print("\nğŸ“‹ Recommandations:")
            for rec in result['recommendations']:
                print(f"  â€¢ {rec}")
        
        # Afficher progression par taille
        if result['analysis']['coverage_progression']:
            print("\nğŸ“ˆ Progression par taille:")
            for size, metrics in result['analysis']['coverage_progression'].items():
                print(f"  {size} dhÄtu: {metrics['best_coverage']:.1f}% "
                      f"(eff: {metrics['best_efficiency']:.1f}%)")

if __name__ == "__main__":
    main()
