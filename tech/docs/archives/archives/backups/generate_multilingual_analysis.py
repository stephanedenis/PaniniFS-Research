#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
G√©n√©rateur de Document Cross-Linguistique pour Corpus Enfant
Cr√©√© des visualisations et analyses pour chaque langue du corpus
"""

import json
import os
import sys
from collections import defaultdict, Counter

# Chemin vers les exp√©riences
EXPERIMENTS_PATH = "/home/stephane/GitHub/PaniniFS-Research/experiments/dhatu"

def load_child_prompts(lang_code):
    """Charge les prompts enfant pour une langue donn√©e"""
    file_path = os.path.join(EXPERIMENTS_PATH, "prompts_child", f"{lang_code}.json")
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return None

def load_typological_sample():
    """Charge les informations typologiques"""
    file_path = os.path.join(EXPERIMENTS_PATH, "typological_sample.json")
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return {"languages": []}

def get_language_families():
    """Retourne la classification par famille linguistique"""
    return {
        'Indo-Europ√©ennes': {
            'en': 'Anglais',
            'fr': 'Fran√ßais', 
            'deu': 'Allemand',
            'spa': 'Espagnol',
            'nld': 'N√©erlandais',
            'hin': 'Hindi'
        },
        'Sino-Tib√©taines': {
            'cmn': 'Chinois Mandarin'
        },
        'Afro-Asiatiques': {
            'arb': 'Arabe',
            'heb': 'H√©breu',
            'hau': 'Hausa'
        },
        'Niger-Congo': {
            'yor': 'Yoruba',
            'swa': 'Swahili',
            'zul': 'Zulu',
            'ewe': 'Ewe'
        },
        'Alta√Øques': {
            'jpn': 'Japonais',
            'kor': 'Cor√©en',
            'tur': 'Turc'
        },
        'Autres': {
            'eus': 'Basque',
            'hun': 'Hongrois',
            'iku': 'Inuktitut'
        }
    }

def analyze_language_coverage(lang_code, data):
    """Analyse la couverture d'une langue par les dhƒÅtu"""
    if not data or 'items' not in data:
        return {
            'coverage': 0,
            'total_sentences': 0,
            'dhatu_usage': {},
            'phenomena': {},
            'gaps': [],
            'strengths': []
        }
    
    total_sentences = len(data['items'])
    dhatu_usage = Counter()
    phenomena = Counter()
    gaps = []
    strengths = []
    
    # Analyse simple bas√©e sur mots-cl√©s (√† am√©liorer avec vrai analyseur)
    dhatu_keywords = {
        'EXIST': ['is', 'am', 'are', 'est', '√™tre', 'existe', 'sein', 'ist', 'ser', 'estar'],
        'RELATE': ['in', 'on', 'at', 'dans', 'sur', 'avec', 'in', 'auf', 'an', 'en', 'con'],
        'COMM': ['say', 'tell', 'talk', 'dire', 'parler', 'sagen', 'sprechen', 'decir', 'hablar'],
        'EVAL': ['big', 'small', 'good', 'bad', 'grand', 'petit', 'bon', 'gro√ü', 'klein', 'gut'],
        'ITER': ['more', 'again', 'encore', 'plus', 'wieder', 'mehr', 'm√°s', 'otra vez'],
        'MODAL': ['can', 'must', 'may', 'peut', 'doit', 'kann', 'muss', 'puede', 'debe'],
        'CAUSE': ['make', 'do', 'faire', 'machen', 'tun', 'hacer', 'causa', 'porque'],
        'FLOW': ['go', 'come', 'move', 'aller', 'venir', 'gehen', 'kommen', 'ir', 'venir'],
        'DECIDE': ['choose', 'pick', 'want', 'choisir', 'vouloir', 'w√§hlen', 'wollen', 'elegir']
    }
    
    sentences_with_dhatu = 0
    
    for item in data['items']:
        sentence = item.get('text', '').lower()
        item_dhatu = set()
        
        # D√©tecter dhƒÅtu dans la phrase
        for dhatu, keywords in dhatu_keywords.items():
            for keyword in keywords:
                if keyword in sentence:
                    dhatu_usage[dhatu] += 1
                    item_dhatu.add(dhatu)
        
        if item_dhatu:
            sentences_with_dhatu += 1
        
        # Analyser ph√©nom√®nes si disponibles
        if 'phenomena' in item:
            for phenomenon in item['phenomena']:
                phenomena[phenomenon] += 1
    
    coverage = sentences_with_dhatu / total_sentences if total_sentences > 0 else 0
    
    # Identifier gaps et strengths
    if coverage < 0.3:
        gaps.append("Couverture g√©n√©rale faible")
    if 'EXIST' not in dhatu_usage:
        gaps.append("Manque expression existence")
    if 'RELATE' not in dhatu_usage:
        gaps.append("Relations spatiales non d√©tect√©es")
    
    if dhatu_usage.get('EXIST', 0) > 3:
        strengths.append("Forte expression existentielle")
    if len(dhatu_usage) > 5:
        strengths.append("Diversit√© dhƒÅtu √©lev√©e")
    
    return {
        'coverage': coverage,
        'total_sentences': total_sentences,
        'dhatu_usage': dict(dhatu_usage),
        'phenomena': dict(phenomena),
        'gaps': gaps,
        'strengths': strengths
    }

def generate_language_analysis(lang_code):
    """G√©n√®re l'analyse compl√®te d'une langue"""
    data = load_child_prompts(lang_code)
    families = get_language_families()
    
    # Trouver nom et famille
    lang_name = lang_code
    family = "Non class√©e"
    
    for fam, langs in families.items():
        if lang_code in langs:
            lang_name = langs[lang_code]
            family = fam
            break
    
    analysis = analyze_language_coverage(lang_code, data)
    
    return {
        'code': lang_code,
        'name': lang_name,
        'family': family,
        'analysis': analysis,
        'raw_data': data
    }

def create_corpus_visualization_document():
    """Cr√©e le document principal avec graphiques et analyses"""
    
    # Obtenir toutes les langues disponibles
    prompts_dir = os.path.join(EXPERIMENTS_PATH, "prompts_child")
    available_langs = []
    
    if os.path.exists(prompts_dir):
        for filename in os.listdir(prompts_dir):
            if filename.endswith('.json') and filename != 'schema.json':
                available_langs.append(filename[:-5])  # Remove .json
    
    available_langs.sort()
    
    # Analyser toutes les langues
    all_analyses = {}
    for lang in available_langs:
        all_analyses[lang] = generate_language_analysis(lang)
    
    # G√©n√©rer le document
    doc_content = f"""# üìä CORPUS MULTILINGUE ENFANT : Validation DhƒÅtu Universels

*Analyse exhaustive de l'ad√©quation du syst√®me PaniniSpeak sur {len(available_langs)} langues*

---

## üéØ **Vue d'Ensemble**

### **Corpus Analys√©**
- **Langues test√©es** : {len(available_langs)}
- **Familles linguistiques** : {len(get_language_families())}
- **Total phrases enfant** : {sum(a['analysis']['total_sentences'] for a in all_analyses.values())}

### **Syst√®me DhƒÅtu Test√©**
```
[EXIST] [RELATE] [COMM] [EVAL] [ITER] [MODAL] [CAUSE] [FLOW] [DECIDE]
```

---

## üåç **Analyse par Famille Linguistique**

"""
    
    families = get_language_families()
    family_stats = defaultdict(list)
    
    # Regrouper par famille
    for lang_code, analysis in all_analyses.items():
        family_stats[analysis['family']].append(analysis)
    
    # G√©n√©rer analyse par famille
    for family, analyses in family_stats.items():
        doc_content += f"""### **{family}**

| Langue | Code | Coverage | Phrases | Top DhƒÅtu | Gaps Identifi√©s |
|--------|------|----------|---------|-----------|-----------------|
"""
        
        for analysis in analyses:
            lang = analysis['analysis']
            top_dhatu = sorted(lang['dhatu_usage'].items(), key=lambda x: x[1], reverse=True)[:3]
            top_dhatu_str = ", ".join([f"{d}({c})" for d, c in top_dhatu]) if top_dhatu else "Aucun"
            gaps_str = "; ".join(lang['gaps'][:2]) if lang['gaps'] else "Aucun majeur"
            
            doc_content += f"| {analysis['name']} | `{analysis['code']}` | {lang['coverage']:.1%} | {lang['total_sentences']} | {top_dhatu_str} | {gaps_str} |\n"
    
    doc_content += "\n---\n\n"
    
    # Analyses d√©taill√©es par langue
    doc_content += "## üìã **Analyses D√©taill√©es par Langue**\n\n"
    
    for lang_code in sorted(available_langs):
        analysis = all_analyses[lang_code]
        lang = analysis['analysis']
        
        doc_content += f"""### **{analysis['name']} ({analysis['code'].upper()})**
*Famille : {analysis['family']}*

#### **M√©triques**
- **Coverage globale** : {lang['coverage']:.1%} ({int(lang['coverage'] * lang['total_sentences'])}/{lang['total_sentences']} phrases)
- **DhƒÅtu d√©tect√©s** : {len(lang['dhatu_usage'])} / 9

#### **Usage DhƒÅtu**
"""
        
        if lang['dhatu_usage']:
            dhatu_sorted = sorted(lang['dhatu_usage'].items(), key=lambda x: x[1], reverse=True)
            for dhatu, count in dhatu_sorted:
                percentage = (count / lang['total_sentences'] * 100) if lang['total_sentences'] > 0 else 0
                doc_content += f"- **{dhatu}** : {count} occurrences ({percentage:.1f}%)\n"
        else:
            doc_content += "- *Aucun dhƒÅtu d√©tect√©*\n"
        
        doc_content += "\n#### **Forces et Faiblesses**\n"
        
        if lang['strengths']:
            doc_content += "**‚úÖ Forces :**\n"
            for strength in lang['strengths']:
                doc_content += f"- {strength}\n"
        
        if lang['gaps']:
            doc_content += "**‚ùå Faiblesses :**\n"
            for gap in lang['gaps']:
                doc_content += f"- {gap}\n"
        
        if not lang['strengths'] and not lang['gaps']:
            doc_content += "*√âvaluation neutre - donn√©es insuffisantes*\n"
        
        # Exemples de phrases si disponibles
        if analysis['raw_data'] and 'items' in analysis['raw_data']:
            doc_content += "\n#### **√âchantillon de Phrases**\n"
            sample_items = analysis['raw_data']['items'][:3]
            for i, item in enumerate(sample_items, 1):
                text = item.get('text', 'Texte non disponible')
                doc_content += f"{i}. *{text}*\n"
        
        doc_content += "\n---\n\n"
    
    # Conclusions globales
    total_coverage = sum(a['analysis']['coverage'] for a in all_analyses.values()) / len(all_analyses)
    best_lang = max(all_analyses.values(), key=lambda x: x['analysis']['coverage'])
    worst_lang = min(all_analyses.values(), key=lambda x: x['analysis']['coverage'])
    
    doc_content += f"""## üéØ **Conclusions Globales**

### **Performance Syst√®me**
- **Coverage moyenne** : {total_coverage:.1%}
- **Meilleure langue** : {best_lang['name']} ({best_lang['analysis']['coverage']:.1%})
- **Plus faible** : {worst_lang['name']} ({worst_lang['analysis']['coverage']:.1%})

### **DhƒÅtu les Plus Universels**
"""
    
    # Compter usage global des dhƒÅtu
    global_dhatu = Counter()
    for analysis in all_analyses.values():
        for dhatu, count in analysis['analysis']['dhatu_usage'].items():
            global_dhatu[dhatu] += count
    
    for dhatu, total_count in global_dhatu.most_common():
        lang_count = sum(1 for a in all_analyses.values() if dhatu in a['analysis']['dhatu_usage'])
        universality = lang_count / len(all_analyses) * 100
        doc_content += f"- **{dhatu}** : {total_count} occurrences, {lang_count}/{len(all_analyses)} langues ({universality:.0f}% universalit√©)\n"
    
    doc_content += f"""
### **Recommandations d'Am√©lioration**

#### **Gaps Syst√®me Identifi√©s**
1. **D√©tection linguistique** : Le syst√®me simple de mots-cl√©s sous-estime la couverture r√©elle
2. **Variations morphologiques** : Langues agglutinantes mal d√©tect√©es
3. **Ordre des mots** : Langues SOV posent des d√©fis

#### **Extensions Propos√©es**
1. **Analyseur morphologique** pour langues complexes
2. **Patterns syntaxiques** sp√©cifiques par famille
3. **Corpus √©tendus** pour validation statistique

---

*Document g√©n√©r√© automatiquement le {__import__('datetime').datetime.now().strftime('%d/%m/%Y √† %H:%M')}*
*Bas√© sur corpus enfant PaniniFS Research*
"""
    
    return doc_content

def create_graphical_summary():
    """Cr√©e un r√©sum√© graphique des r√©sultats"""
    # Obtenir toutes les langues disponibles
    prompts_dir = os.path.join(EXPERIMENTS_PATH, "prompts_child")
    available_langs = []
    
    if os.path.exists(prompts_dir):
        for filename in os.listdir(prompts_dir):
            if filename.endswith('.json') and filename != 'schema.json':
                available_langs.append(filename[:-5])
    
    # Analyser toutes les langues
    all_analyses = {}
    for lang in available_langs:
        all_analyses[lang] = generate_language_analysis(lang)
    
    # Cr√©er visualisation ASCII simple
    graph_content = """# üìà GRAPHIQUE : Coverage DhƒÅtu par Langue

## R√©partition des Coverages

```
Coverage (%)
  100 |
      |
   80 |
      |
   60 |
      |
   40 |
      |
   20 |  ‚óè
      |     ‚óè
    0 |________‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
      """
    
    # Ajouter les langues tri√©es par coverage
    sorted_langs = sorted(all_analyses.items(), key=lambda x: x[1]['analysis']['coverage'], reverse=True)
    
    graph_content += "\n\n## Classement par Coverage\n\n"
    for i, (lang_code, analysis) in enumerate(sorted_langs):
        coverage = analysis['analysis']['coverage']
        bar_length = int(coverage * 50)  # Barre sur 50 caract√®res
        bar = "‚ñà" * bar_length + "‚ñë" * (50 - bar_length)
        graph_content += f"{i+1:2d}. {analysis['name']:<15} {coverage:5.1%} |{bar}|\n"
    
    return graph_content

def main():
    """Fonction principale"""
    
    print("üîÑ G√©n√©ration du document d'analyse corpus multilingue...")
    
    # Cr√©er le document principal
    main_doc = create_corpus_visualization_document()
    
    # Cr√©er le graphique
    graph_doc = create_graphical_summary()
    
    # Sauvegarder les documents
    output_dir = "/home/stephane/GitHub/PaniniFS-Research/data/references_cache"
    
    main_file = os.path.join(output_dir, "CORPUS_MULTILINGUE_ANALYSE_COMPLETE.md")
    graph_file = os.path.join(output_dir, "CORPUS_MULTILINGUE_GRAPHIQUES.md")
    
    with open(main_file, 'w', encoding='utf-8') as f:
        f.write(main_doc)
    
    with open(graph_file, 'w', encoding='utf-8') as f:
        f.write(graph_doc)
    
    print(f"‚úÖ Documents g√©n√©r√©s :")
    print(f"   üìÑ Analyse compl√®te : {main_file}")
    print(f"   üìä Graphiques : {graph_file}")

if __name__ == "__main__":
    main()
