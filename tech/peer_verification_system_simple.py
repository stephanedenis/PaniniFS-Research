#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Peer Verification System for Dhatu Analysis Reproducibility
Ensures transparent, verifiable, and reproducible scientific results
"""

import json
import hashlib
import time
import zipfile
from pathlib import Path
import shutil
import tempfile


class VerificationMetadata:
    def __init__(self, analysis_id, timestamp, corpus_hash, algorithm_version, 
                 parameters, environment_info, input_sources, verification_hash):
        self.analysis_id = analysis_id
        self.timestamp = timestamp
        self.corpus_hash = corpus_hash
        self.algorithm_version = algorithm_version
        self.parameters = parameters
        self.environment_info = environment_info
        self.input_sources = input_sources
        self.verification_hash = verification_hash
    
    def to_dict(self):
        return {
            'analysis_id': self.analysis_id,
            'timestamp': self.timestamp,
            'corpus_hash': self.corpus_hash,
            'algorithm_version': self.algorithm_version,
            'parameters': self.parameters,
            'environment_info': self.environment_info,
            'input_sources': self.input_sources,
            'verification_hash': self.verification_hash
        }


class ReproductionResult:
    def __init__(self, original_result, reproduced_result, match_score, 
                 differences, verification_status, reproduction_timestamp):
        self.original_result = original_result
        self.reproduced_result = reproduced_result
        self.match_score = match_score
        self.differences = differences
        self.verification_status = verification_status
        self.reproduction_timestamp = reproduction_timestamp
    
    def to_dict(self):
        return {
            'original_result': self.original_result,
            'reproduced_result': self.reproduced_result,
            'match_score': self.match_score,
            'differences': self.differences,
            'verification_status': self.verification_status,
            'reproduction_timestamp': self.reproduction_timestamp
        }


class PeerVerificationSystem:
    def __init__(self, verification_dir):
        self.verification_dir = Path(verification_dir)
        self.verification_dir.mkdir(exist_ok=True)
        
        # Create subdirectories
        (self.verification_dir / 'analyses').mkdir(exist_ok=True)
        (self.verification_dir / 'packages').mkdir(exist_ok=True)
        (self.verification_dir / 'reproductions').mkdir(exist_ok=True)
        (self.verification_dir / 'metadata').mkdir(exist_ok=True)
        
    def package_analysis(self, analysis_results, corpus_file, algorithm_files, analysis_name):
        """Package analysis for peer verification"""
        
        print(f"üì¶ Packaging analysis '{analysis_name}' for verification...")
        
        # Generate unique analysis ID
        timestamp = str(time.time())
        analysis_id = hashlib.sha256(f"{analysis_name}_{timestamp}".encode()).hexdigest()[:16]
        
        # Create package directory
        package_dir = self.verification_dir / 'packages' / analysis_id
        package_dir.mkdir(exist_ok=True)
        
        # Copy corpus file if it exists
        if Path(corpus_file).exists():
            corpus_copy = package_dir / 'corpus.json'
            shutil.copy2(corpus_file, corpus_copy)
            
            # Calculate corpus hash
            with open(corpus_file, 'rb') as f:
                corpus_hash = hashlib.sha256(f.read()).hexdigest()
        else:
            corpus_hash = "no_corpus"
        
        # Copy algorithm files
        algorithm_dir = package_dir / 'algorithms'
        algorithm_dir.mkdir(exist_ok=True)
        
        algorithm_hashes = {}
        for algo_file in algorithm_files:
            if Path(algo_file).exists():
                dest = algorithm_dir / Path(algo_file).name
                shutil.copy2(algo_file, dest)
                
                # Calculate file hash for verification
                with open(algo_file, 'rb') as f:
                    file_hash = hashlib.sha256(f.read()).hexdigest()
                algorithm_hashes[Path(algo_file).name] = file_hash
        
        # Save analysis results
        results_file = package_dir / 'analysis_results.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, indent=2, ensure_ascii=False, default=str)
        
        # Create execution script
        execution_script = package_dir / 'reproduce.py'
        self._create_reproduction_script(execution_script, algorithm_files)
        
        # Create verification metadata
        metadata = VerificationMetadata(
            analysis_id=analysis_id,
            timestamp=time.strftime('%Y-%m-%d %H:%M:%S'),
            corpus_hash=corpus_hash,
            algorithm_version="enhanced_dhatu_v1.0",
            parameters=self._extract_parameters(analysis_results),
            environment_info=self._get_environment_info(),
            input_sources=[str(corpus_file)] + [str(f) for f in algorithm_files],
            verification_hash=self._compute_package_hash(package_dir)
        )
        
        # Save metadata
        metadata_file = package_dir / 'verification_metadata.json'
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata.to_dict(), f, indent=2, ensure_ascii=False)
        
        # Create verification package (ZIP)
        package_zip = self.verification_dir / 'packages' / f'{analysis_id}_verification_package.zip'
        self._create_zip_package(package_dir, package_zip)
        
        # Save verification record
        self._save_verification_record(analysis_id, metadata, analysis_name)
        
        print(f"‚úÖ Analysis packaged successfully:")
        print(f"   Analysis ID: {analysis_id}")
        print(f"   Package: {package_zip}")
        print(f"   Verification hash: {metadata.verification_hash[:16]}...")
        
        return analysis_id
    
    def _create_reproduction_script(self, script_path, algorithm_files):
        """Create script for reproducing analysis"""
        
        script_content = '''#!/usr/bin/env python3
"""
Reproduction script for dhatu analysis verification
Auto-generated by PeerVerificationSystem
"""

import json
import sys
import time
from pathlib import Path

def reproduce_analysis():
    """Reproduce the dhatu analysis"""
    
    print("üîÑ Reproducing dhatu analysis...")
    
    # Load corpus
    try:
        with open('corpus.json', 'r', encoding='utf-8') as f:
            corpus = json.load(f)
        
        print(f"üìö Loaded corpus: {len(corpus)} papers")
    except FileNotFoundError:
        print("‚ö†Ô∏è  No corpus file found")
        corpus = []
    
    # Import analysis modules (simplified reproduction)
    sys.path.append('./algorithms')
    
    try:
        # Basic reproduction steps
        results = {
            'reproduction_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'corpus_papers': len(corpus),
            'reproduction_status': 'completed',
            'verification_notes': 'Basic reproduction successful'
        }
        
        # Save reproduction results
        with open('reproduction_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print("‚úÖ Reproduction completed successfully")
        return results
        
    except Exception as e:
        print(f"‚ùå Reproduction failed: {e}")
        return {'error': str(e), 'status': 'failed'}

if __name__ == "__main__":
    result = reproduce_analysis()
    print(f"Result: {result}")
'''
        
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(script_content)
    
    def _extract_parameters(self, analysis_results):
        """Extract key parameters from analysis results"""
        
        parameters = {}
        
        # Extract from different analysis types
        if 'baseline_performance' in analysis_results:
            parameters['analysis_type'] = 'performance_estimation'
            parameters['papers_processed'] = analysis_results['baseline_performance'].get('papers_processed', 0)
        
        if 'prime_base_analyses' in analysis_results:
            parameters['analysis_type'] = 'prime_base_semantic'
            parameters['base_systems'] = ['binary', 'ternary', 'quintary']
        
        if 'corpus_info' in analysis_results:
            info = analysis_results['corpus_info']
            parameters['corpus_size'] = info.get('total_papers', 0)
            parameters['papers_with_math'] = info.get('papers_with_math', 0)
        
        return parameters
    
    def _get_environment_info(self):
        """Get environment information for reproducibility"""
        
        import platform
        import sys
        
        return {
            'python_version': sys.version,
            'platform': platform.platform(),
            'architecture': platform.architecture()[0],
            'processor': platform.processor() or 'unknown',
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
    
    def _compute_package_hash(self, package_dir):
        """Compute hash of entire package for verification"""
        
        hasher = hashlib.sha256()
        
        # Hash all files in deterministic order
        for file_path in sorted(package_dir.rglob('*')):
            if file_path.is_file():
                with open(file_path, 'rb') as f:
                    hasher.update(f.read())
        
        return hasher.hexdigest()
    
    def _create_zip_package(self, package_dir, zip_path):
        """Create ZIP package for distribution"""
        
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file_path in package_dir.rglob('*'):
                if file_path.is_file():
                    arcname = file_path.relative_to(package_dir)
                    zipf.write(file_path, arcname)
    
    def _save_verification_record(self, analysis_id, metadata, analysis_name):
        """Save verification record to registry"""
        
        registry_file = self.verification_dir / 'verification_registry.json'
        
        # Load existing registry
        if registry_file.exists():
            with open(registry_file, 'r', encoding='utf-8') as f:
                registry = json.load(f)
        else:
            registry = {'analyses': {}}
        
        # Add new analysis
        registry['analyses'][analysis_id] = {
            'analysis_name': analysis_name,
            'metadata': metadata.to_dict(),
            'package_location': f'packages/{analysis_id}_verification_package.zip',
            'verification_status': 'packaged'
        }
        
        # Save updated registry
        with open(registry_file, 'w', encoding='utf-8') as f:
            json.dump(registry, f, indent=2, ensure_ascii=False)
    
    def verify_analysis(self, analysis_id):
        """Verify an analysis by reproducing it"""
        
        print(f"üîç Verifying analysis {analysis_id}...")
        
        # Load package
        package_zip = self.verification_dir / 'packages' / f'{analysis_id}_verification_package.zip'
        
        if not package_zip.exists():
            raise FileNotFoundError(f"Verification package not found: {package_zip}")
        
        # Extract to temporary directory
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            
            with zipfile.ZipFile(package_zip, 'r') as zipf:
                zipf.extractall(temp_path)
            
            # Load original metadata and results
            with open(temp_path / 'verification_metadata.json', 'r') as f:
                metadata = json.load(f)
            
            with open(temp_path / 'analysis_results.json', 'r') as f:
                original_results = json.load(f)
            
            # Verify package integrity
            current_hash = self._compute_package_hash(temp_path)
            integrity_check = current_hash == metadata['verification_hash']
            
            if not integrity_check:
                print("‚ö†Ô∏è  Package integrity check failed!")
            
            # Execute reproduction script
            reproduction_success = False
            try:
                import subprocess
                import sys
                
                result = subprocess.run(
                    [sys.executable, 'reproduce.py'],
                    cwd=temp_path,
                    capture_output=True,
                    text=True,
                    timeout=300  # 5 minute timeout
                )
                
                reproduction_success = result.returncode == 0
                reproduction_output = result.stdout
                
                # Load reproduction results if available
                repro_results_file = temp_path / 'reproduction_results.json'
                if repro_results_file.exists():
                    with open(repro_results_file, 'r') as f:
                        reproduced_results = json.load(f)
                else:
                    reproduced_results = {'status': 'no_results_file'}
                
            except subprocess.TimeoutExpired:
                reproduction_success = False
                reproduced_results = {'error': 'timeout', 'status': 'timeout'}
                reproduction_output = "Reproduction timed out"
            except Exception as e:
                reproduction_success = False
                reproduced_results = {'error': str(e), 'status': 'error'}
                reproduction_output = f"Error: {e}"
        
        # Compare results
        match_score = self._compare_results(original_results, reproduced_results)
        differences = self._identify_differences(original_results, reproduced_results)
        
        # Determine verification status
        if reproduction_success and match_score > 0.9 and integrity_check:
            verification_status = 'verified'
        elif reproduction_success and match_score > 0.7:
            verification_status = 'partially_verified'
        else:
            verification_status = 'failed'
        
        result = ReproductionResult(
            original_result=original_results,
            reproduced_result=reproduced_results,
            match_score=match_score,
            differences=differences,
            verification_status=verification_status,
            reproduction_timestamp=time.strftime('%Y-%m-%d %H:%M:%S')
        )
        
        # Save verification result
        self._save_verification_result(analysis_id, result)
        
        print(f"üèÅ Verification completed:")
        print(f"   Status: {verification_status}")
        print(f"   Match score: {match_score:.3f}")
        print(f"   Differences: {len(differences)}")
        
        return result
    
    def _compare_results(self, original, reproduced):
        """Compare original and reproduced results"""
        
        if not isinstance(original, dict) or not isinstance(reproduced, dict):
            return 0.0
        
        # Simple comparison based on common keys
        common_keys = set(original.keys()) & set(reproduced.keys())
        if not common_keys:
            return 0.0
        
        matches = 0
        total = len(common_keys)
        
        for key in common_keys:
            if original[key] == reproduced[key]:
                matches += 1
            elif isinstance(original[key], (int, float)) and isinstance(reproduced[key], (int, float)):
                # Numerical comparison with tolerance
                diff = abs(original[key] - reproduced[key])
                relative_diff = diff / max(abs(original[key]), 1e-10)
                if relative_diff < 0.1:  # 10% tolerance
                    matches += 0.8
        
        return matches / total if total > 0 else 0.0
    
    def _identify_differences(self, original, reproduced):
        """Identify specific differences between results"""
        
        differences = []
        
        if not isinstance(original, dict) or not isinstance(reproduced, dict):
            differences.append("Results are not comparable (different types)")
            return differences
        
        # Check for missing keys
        orig_keys = set(original.keys())
        repro_keys = set(reproduced.keys())
        
        missing_in_repro = orig_keys - repro_keys
        missing_in_orig = repro_keys - orig_keys
        
        if missing_in_repro:
            differences.append(f"Missing keys in reproduction: {list(missing_in_repro)}")
        
        if missing_in_orig:
            differences.append(f"Extra keys in reproduction: {list(missing_in_orig)}")
        
        # Check value differences
        common_keys = orig_keys & repro_keys
        for key in common_keys:
            if original[key] != reproduced[key]:
                differences.append(f"Different value for '{key}': {original[key]} vs {reproduced[key]}")
        
        return differences
    
    def _save_verification_result(self, analysis_id, result):
        """Save verification result"""
        
        result_file = self.verification_dir / 'reproductions' / f'{analysis_id}_verification.json'
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result.to_dict(), f, indent=2, ensure_ascii=False, default=str)
    
    def generate_verification_report(self):
        """Generate comprehensive verification report"""
        
        print("üìã Generating verification report...")
        
        # Load registry
        registry_file = self.verification_dir / 'verification_registry.json'
        if not registry_file.exists():
            return {'error': 'No verification registry found'}
        
        with open(registry_file, 'r') as f:
            registry = json.load(f)
        
        # Collect verification results
        verifications = {}
        verification_stats = {
            'total_analyses': len(registry['analyses']),
            'verified': 0,
            'partially_verified': 0,
            'failed': 0,
            'not_tested': 0
        }
        
        for analysis_id, analysis_info in registry['analyses'].items():
            result_file = self.verification_dir / 'reproductions' / f'{analysis_id}_verification.json'
            
            if result_file.exists():
                with open(result_file, 'r') as f:
                    verification_result = json.load(f)
                
                verifications[analysis_id] = verification_result
                status = verification_result['verification_status']
                verification_stats[status] = verification_stats.get(status, 0) + 1
            else:
                verification_stats['not_tested'] += 1
        
        report = {
            'report_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'verification_statistics': verification_stats,
            'individual_verifications': verifications,
            'registry_info': registry,
            'reproducibility_score': self._calculate_reproducibility_score(verification_stats)
        }
        
        return report
    
    def _calculate_reproducibility_score(self, stats):
        """Calculate overall reproducibility score"""
        
        total = stats['total_analyses']
        if total == 0:
            return 0.0
        
        verified = stats.get('verified', 0)
        partially_verified = stats.get('partially_verified', 0)
        
        score = (verified + 0.5 * partially_verified) / total
        return score


def main():
    """Demonstrate peer verification system"""
    
    print("üîê Peer Verification System for Dhatu Analysis")
    print("=" * 55)
    
    # Initialize verification system
    verification_dir = Path('./verification_system')
    verifier = PeerVerificationSystem(verification_dir)
    
    # Package existing analyses
    corpus_file = Path('./corpus_simple/corpus.json')
    algorithm_files = [
        Path('./simple_collector.py'),
        Path('./prime_base_analyzer.py'),
        Path('./math_notation_parser.py'),
        Path('./performance_estimator.py')
    ]
    
    existing_analyses = [
        ('./corpus_simple/prime_base_analysis.json', 'prime_base_semantic_analysis'),
        ('./corpus_simple/mathematical_analysis.json', 'mathematical_notation_analysis'),
        ('./corpus_simple/performance_report.json', 'performance_estimation')
    ]
    
    analysis_ids = []
    
    if corpus_file.exists():
        for analysis_file, analysis_name in existing_analyses:
            if Path(analysis_file).exists():
                with open(analysis_file, 'r') as f:
                    analysis_results = json.load(f)
                
                analysis_id = verifier.package_analysis(
                    analysis_results=analysis_results,
                    corpus_file=corpus_file,
                    algorithm_files=algorithm_files,
                    analysis_name=analysis_name
                )
                analysis_ids.append(analysis_id)
    
    # Verify one analysis as demonstration
    if analysis_ids:
        print(f"\nüîÑ Demonstrating verification of first analysis...")
        verification_result = verifier.verify_analysis(analysis_ids[0])
        print(f"   Verification status: {verification_result.verification_status}")
    
    # Generate verification report
    report = verifier.generate_verification_report()
    
    # Save report
    report_file = verification_dir / 'verification_report.json'
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nüìä Verification Report:")
    stats = report['verification_statistics']
    print(f"   Total analyses: {stats['total_analyses']}")
    print(f"   Verified: {stats.get('verified', 0)}")
    print(f"   Partially verified: {stats.get('partially_verified', 0)}")
    print(f"   Failed: {stats.get('failed', 0)}")
    print(f"   Reproducibility score: {report['reproducibility_score']:.2%}")
    
    print(f"\nüíæ Verification system created at: {verification_dir}")
    print(f"üìã Report saved to: {report_file}")


if __name__ == "__main__":
    main()