#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPU Memory Optimization and Streaming for Large-Scale DhƒÅtu Processing
Implements intelligent batching and memory management for massive corpora
"""

import numpy as np
import json
import time
import psutil
import gc
from pathlib import Path
from typing import Dict, List, Any, Iterator, Tuple, Optional
from dataclasses import dataclass
import hashlib
import threading
from queue import Queue, Empty

try:
    import cupy as cp
    import cupy.cuda.memory_pool as mempool
    GPU_AVAILABLE = True
    print("‚úÖ GPU acceleration available with CuPy")
except ImportError:
    import numpy as cp
    GPU_AVAILABLE = False
    print("‚ö†Ô∏è  GPU not available - using CPU with optimized memory management")


@dataclass
class MemoryProfile:
    """Memory usage profile for optimization"""
    total_memory_gb: float
    available_memory_gb: float
    gpu_memory_gb: float
    gpu_available_gb: float
    optimal_batch_size: int
    recommended_streaming: bool


@dataclass
class StreamingConfig:
    """Configuration for streaming processing"""
    batch_size: int
    buffer_size: int
    prefetch_batches: int
    memory_limit_gb: float
    use_gpu_streaming: bool
    compression_enabled: bool


class MemoryOptimizer:
    """Intelligent memory optimization for dhƒÅtu processing"""
    
    def __init__(self):
        self.system_memory = psutil.virtual_memory()
        self.gpu_memory = self._get_gpu_memory()
        self.memory_profile = self._analyze_memory_profile()
        
    def _get_gpu_memory(self) -> Dict[str, float]:
        """Get GPU memory information"""
        
        if GPU_AVAILABLE:
            try:
                # Get GPU memory info
                meminfo = cp.cuda.MemoryInfo()
                total_gpu = meminfo.total / (1024**3)  # Convert to GB
                free_gpu = meminfo.free / (1024**3)
                
                return {
                    'total_gb': total_gpu,
                    'free_gb': free_gpu,
                    'used_gb': total_gpu - free_gpu
                }
            except Exception:
                pass
        
        return {'total_gb': 0.0, 'free_gb': 0.0, 'used_gb': 0.0}
    
    def _analyze_memory_profile(self) -> MemoryProfile:
        """Analyze system memory and recommend optimal configuration"""
        
        # System memory analysis
        total_ram_gb = self.system_memory.total / (1024**3)
        available_ram_gb = self.system_memory.available / (1024**3)
        
        # GPU memory analysis
        gpu_total = self.gpu_memory['total_gb']
        gpu_available = self.gpu_memory['free_gb']
        
        # Calculate optimal batch size based on memory constraints
        # Assume each text uses ~1KB for dhƒÅtu processing
        text_memory_kb = 1.0
        
        if GPU_AVAILABLE and gpu_available > 2.0:
            # GPU processing - use 80% of available GPU memory
            usable_memory_gb = gpu_available * 0.8
            optimal_batch_size = int((usable_memory_gb * 1024 * 1024) / text_memory_kb)
            recommended_streaming = optimal_batch_size < 10000
        else:
            # CPU processing - use 50% of available RAM
            usable_memory_gb = available_ram_gb * 0.5
            optimal_batch_size = int((usable_memory_gb * 1024 * 1024) / text_memory_kb)
            recommended_streaming = optimal_batch_size < 5000
        
        # Clamp batch size to reasonable bounds
        optimal_batch_size = max(100, min(optimal_batch_size, 50000))
        
        return MemoryProfile(
            total_memory_gb=total_ram_gb,
            available_memory_gb=available_ram_gb,
            gpu_memory_gb=gpu_total,
            gpu_available_gb=gpu_available,
            optimal_batch_size=optimal_batch_size,
            recommended_streaming=recommended_streaming
        )
    
    def create_streaming_config(self, corpus_size: int) -> StreamingConfig:
        """Create optimal streaming configuration for corpus size"""
        
        profile = self.memory_profile
        
        # Adaptive batch sizing based on corpus size
        if corpus_size <= 1000:
            batch_size = min(corpus_size, profile.optimal_batch_size)
            buffer_size = 2
            prefetch_batches = 1
        elif corpus_size <= 10000:
            batch_size = min(1000, profile.optimal_batch_size // 2)
            buffer_size = 4
            prefetch_batches = 2
        elif corpus_size <= 100000:
            batch_size = min(2000, profile.optimal_batch_size // 4)
            buffer_size = 8
            prefetch_batches = 3
        else:
            # Large corpus - aggressive optimization
            batch_size = min(5000, profile.optimal_batch_size // 8)
            buffer_size = 16
            prefetch_batches = 4
        
        # Memory limit for processing
        if GPU_AVAILABLE:
            memory_limit = profile.gpu_available_gb * 0.9
        else:
            memory_limit = profile.available_memory_gb * 0.7
        
        return StreamingConfig(
            batch_size=batch_size,
            buffer_size=buffer_size,
            prefetch_batches=prefetch_batches,
            memory_limit_gb=memory_limit,
            use_gpu_streaming=GPU_AVAILABLE,
            compression_enabled=corpus_size > 50000
        )


class StreamingProcessor:
    """Streaming processor for large corpus with memory optimization"""
    
    def __init__(self, config: StreamingConfig):
        self.config = config
        self.memory_optimizer = MemoryOptimizer()
        self.processing_queue = Queue(maxsize=config.buffer_size)
        self.result_queue = Queue(maxsize=config.buffer_size)
        self.stop_event = threading.Event()
        
        # Initialize GPU memory pool if available
        if GPU_AVAILABLE:
            self.gpu_pool = mempool.MemoryPool()
            cp.cuda.set_allocator(self.gpu_pool.malloc)
        
        # DhƒÅtu processing constants
        self.dhatu_names = ['RELATE', 'MODAL', 'EXIST', 'EVAL', 'COMM', 
                           'CAUSE', 'ITER', 'DECIDE', 'FEEL']
        self.prime_numbers = [2, 3, 5, 7, 11]
        
    def create_text_batches(self, texts: List[str]) -> Iterator[List[str]]:
        """Create batches from text corpus with memory-aware sizing"""
        
        batch_size = self.config.batch_size
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            yield batch
    
    def process_batch_dhatu_vectorization(self, texts: List[str]) -> np.ndarray:
        """Process dhƒÅtu vectorization for a batch with memory optimization"""
        
        num_texts = len(texts)
        num_dhatus = len(self.dhatu_names)
        
        if self.config.use_gpu_streaming and GPU_AVAILABLE:
            # GPU processing with memory management
            vectors = cp.zeros((num_texts, num_dhatus), dtype=cp.float32)
            
            # Process in mini-batches to avoid GPU memory overflow
            mini_batch_size = min(500, num_texts)
            
            for i in range(0, num_texts, mini_batch_size):
                end_idx = min(i + mini_batch_size, num_texts)
                mini_batch = texts[i:end_idx]
                
                # Process mini-batch
                for j, text in enumerate(mini_batch):
                    for k, dhatu in enumerate(self.dhatu_names):
                        combined = f"{dhatu}::{text}".encode('utf-8')
                        hash_obj = hashlib.sha256(combined)
                        hash_bytes = hash_obj.digest()
                        
                        weight = (hash_bytes[0] + hash_bytes[5] + hash_bytes[13] + 
                                 hash_bytes[21] + hash_bytes[29]) / 1024.0
                        vectors[i + j, k] = weight
                
                # Clear GPU cache periodically
                if i % (mini_batch_size * 10) == 0:
                    cp.cuda.Stream.null.synchronize()
                    self.gpu_pool.free_all_blocks()
            
            # L1 normalization on GPU
            row_sums = cp.sum(vectors, axis=1, keepdims=True)
            row_sums = cp.where(row_sums == 0, 1, row_sums)
            vectors = vectors / row_sums
            
            # Convert to CPU for return
            result = vectors.get()
            
            # Clear GPU memory
            del vectors
            cp.cuda.Stream.null.synchronize()
            self.gpu_pool.free_all_blocks()
            
        else:
            # CPU processing with memory optimization
            vectors = np.zeros((num_texts, num_dhatus), dtype=np.float32)
            
            # Process in chunks to manage memory
            chunk_size = min(100, num_texts)
            
            for i in range(0, num_texts, chunk_size):
                end_idx = min(i + chunk_size, num_texts)
                
                for j in range(i, end_idx):
                    text = texts[j]
                    for k, dhatu in enumerate(self.dhatu_names):
                        combined = f"{dhatu}::{text}".encode('utf-8')
                        hash_obj = hashlib.sha256(combined)
                        hash_bytes = hash_obj.digest()
                        
                        weight = (hash_bytes[0] + hash_bytes[5] + hash_bytes[13] + 
                                 hash_bytes[21] + hash_bytes[29]) / 1024.0
                        vectors[j, k] = weight
                
                # Force garbage collection periodically
                if i % (chunk_size * 20) == 0:
                    gc.collect()
            
            # L1 normalization
            row_sums = np.sum(vectors, axis=1, keepdims=True)
            row_sums = np.where(row_sums == 0, 1, row_sums)
            vectors = vectors / row_sums
            
            result = vectors
        
        return result
    
    def process_batch_prime_bases(self, dhatu_vectors: np.ndarray) -> np.ndarray:
        """Process prime base computation with memory optimization"""
        
        num_documents, num_dhatus = dhatu_vectors.shape
        num_bases = len(self.prime_numbers)
        
        if self.config.use_gpu_streaming and GPU_AVAILABLE:
            # GPU processing
            vectors_gpu = cp.asarray(dhatu_vectors)
            prime_bases = cp.zeros((num_documents, num_bases), dtype=cp.float32)
            
            # Process each prime base
            for base_idx, prime in enumerate(self.prime_numbers):
                powers = cp.arange(num_dhatus, dtype=cp.float32)
                prime_powers = cp.power(prime, powers % prime)
                
                weighted_activations = vectors_gpu * prime_powers[cp.newaxis, :]
                base_activations = cp.sum(weighted_activations, axis=1)
                
                prime_bases[:, base_idx] = base_activations
            
            result = prime_bases.get()
            
            # Clean up GPU memory
            del vectors_gpu, prime_bases
            cp.cuda.Stream.null.synchronize()
            self.gpu_pool.free_all_blocks()
            
        else:
            # CPU processing
            prime_bases = np.zeros((num_documents, num_bases), dtype=np.float32)
            
            for base_idx, prime in enumerate(self.prime_numbers):
                powers = np.arange(num_dhatus, dtype=np.float32)
                prime_powers = np.power(prime, powers % prime)
                
                weighted_activations = dhatu_vectors * prime_powers[np.newaxis, :]
                base_activations = np.sum(weighted_activations, axis=1)
                
                prime_bases[:, base_idx] = base_activations
            
            result = prime_bases
        
        return result
    
    def process_streaming_corpus(self, texts: List[str]) -> Dict[str, Any]:
        """Process large corpus using streaming with memory optimization"""
        
        print(f"\nüåä STREAMING PROCESSING WITH MEMORY OPTIMIZATION")
        print("=" * 60)
        
        start_time = time.time()
        total_texts = len(texts)
        
        print(f"üìä Processing {total_texts} texts")
        print(f"‚öôÔ∏è  Batch size: {self.config.batch_size}")
        print(f"üîß Buffer size: {self.config.buffer_size}")
        print(f"üíæ Memory limit: {self.config.memory_limit_gb:.1f}GB")
        print(f"üî• GPU streaming: {self.config.use_gpu_streaming}")
        
        # Initialize results containers
        all_dhatu_vectors = []
        all_prime_bases = []
        processing_stats = {
            'batches_processed': 0,
            'texts_processed': 0,
            'memory_peaks': [],
            'batch_times': []
        }
        
        # Create batches and process
        batches = list(self.create_text_batches(texts))
        total_batches = len(batches)
        
        for batch_idx, batch in enumerate(batches):
            batch_start = time.time()
            
            # Monitor memory before processing
            memory_before = psutil.virtual_memory().percent
            
            # Process dhƒÅtu vectorization
            dhatu_vectors = self.process_batch_dhatu_vectorization(batch)
            all_dhatu_vectors.append(dhatu_vectors)
            
            # Process prime bases
            prime_bases = self.process_batch_prime_bases(dhatu_vectors)
            all_prime_bases.append(prime_bases)
            
            # Monitor memory after processing
            memory_after = psutil.virtual_memory().percent
            memory_peak = max(memory_before, memory_after)
            
            batch_time = time.time() - batch_start
            
            # Update statistics
            processing_stats['batches_processed'] += 1
            processing_stats['texts_processed'] += len(batch)
            processing_stats['memory_peaks'].append(memory_peak)
            processing_stats['batch_times'].append(batch_time)
            
            # Progress reporting
            if (batch_idx + 1) % max(1, total_batches // 10) == 0:
                progress = ((batch_idx + 1) / total_batches) * 100
                avg_time = np.mean(processing_stats['batch_times'])
                print(f"   üìà Progress: {progress:.1f}% "
                      f"({batch_idx + 1}/{total_batches} batches, "
                      f"avg {avg_time:.3f}s/batch)")
            
            # Memory management - force cleanup if memory usage is high
            if memory_peak > 80.0:
                gc.collect()
                if GPU_AVAILABLE:
                    cp.cuda.Stream.null.synchronize()
                    self.gpu_pool.free_all_blocks()
        
        # Aggregate results
        final_dhatu_vectors = np.vstack(all_dhatu_vectors) if all_dhatu_vectors else np.array([])
        final_prime_bases = np.vstack(all_prime_bases) if all_prime_bases else np.array([])
        
        total_time = time.time() - start_time
        
        # Calculate final statistics
        results = {
            'total_texts': total_texts,
            'total_batches': total_batches,
            'processing_time': total_time,
            'texts_per_second': total_texts / total_time,
            'average_batch_time': np.mean(processing_stats['batch_times']),
            'peak_memory_usage': max(processing_stats['memory_peaks']),
            'memory_efficiency': {
                'memory_limit_gb': self.config.memory_limit_gb,
                'peak_usage_percent': max(processing_stats['memory_peaks']),
                'average_usage_percent': np.mean(processing_stats['memory_peaks']),
                'memory_stable': np.std(processing_stats['memory_peaks']) < 5.0
            },
            'streaming_config': {
                'batch_size': self.config.batch_size,
                'buffer_size': self.config.buffer_size,
                'gpu_enabled': self.config.use_gpu_streaming,
                'compression_enabled': self.config.compression_enabled
            },
            'output_shapes': {
                'dhatu_vectors': final_dhatu_vectors.shape,
                'prime_bases': final_prime_bases.shape
            },
            'performance_metrics': {
                'cpu_efficiency': total_texts / (total_time * psutil.cpu_count()),
                'memory_throughput_mb_s': (final_dhatu_vectors.nbytes + final_prime_bases.nbytes) / (1024 * 1024) / total_time,
                'gpu_utilization': self.config.use_gpu_streaming
            }
        }
        
        return results


def create_massive_test_corpus(size=10000) -> List[str]:
    """Create massive test corpus for memory optimization testing"""
    
    base_patterns = [
        "Advanced computational {domain} demonstrates {technique} improvements in {metric} through {method}.",
        "Research in {domain} reveals {pattern} patterns in {data_type} using {analysis_method}.",
        "Statistical {analysis} shows {correlation} relationships between {variable1} and {variable2}.",
        "Machine learning {algorithm} achieves {performance} accuracy on {dataset} classification tasks.",
        "Quantum {technique} provides {advantage} for {application} in {domain} computing.",
        "Mathematical {formulation} establishes {property} for {system} under {conditions}.",
        "Experimental {methodology} validates {hypothesis} through {measurement} of {phenomena}.",
        "Distributed {architecture} enables {capability} processing of {workload} across {infrastructure}.",
        "Optimization {algorithm} minimizes {objective} subject to {constraints} in {problem_space}.",
        "Scientific {investigation} integrates {approach1} and {approach2} to understand {phenomenon}."
    ]
    
    domain_terms = ['physics', 'chemistry', 'biology', 'mathematics', 'computer science', 'engineering']
    technique_terms = ['algorithmic', 'computational', 'analytical', 'experimental', 'theoretical']
    metric_terms = ['efficiency', 'accuracy', 'performance', 'throughput', 'precision']
    
    test_corpus = []
    for i in range(size):
        pattern = base_patterns[i % len(base_patterns)]
        
        # Fill in template variables
        filled_text = pattern.format(
            domain=domain_terms[i % len(domain_terms)],
            technique=technique_terms[i % len(technique_terms)],
            metric=metric_terms[i % len(metric_terms)],
            method=f"method_{i % 20}",
            pattern=f"pattern_{i % 15}",
            data_type=f"data_{i % 10}",
            analysis_method=f"analysis_{i % 12}",
            analysis=f"analysis_{i % 8}",
            correlation=f"correlation_{i % 6}",
            variable1=f"var1_{i % 7}",
            variable2=f"var2_{i % 9}",
            algorithm=f"algorithm_{i % 14}",
            performance=f"{85 + (i % 15)}%",
            dataset=f"dataset_{i % 11}",
            advantage=f"advantage_{i % 5}",
            application=f"app_{i % 13}",
            formulation=f"formulation_{i % 4}",
            property=f"property_{i % 8}",
            system=f"system_{i % 6}",
            conditions=f"conditions_{i % 7}",
            methodology=f"methodology_{i % 5}",
            hypothesis=f"hypothesis_{i % 6}",
            measurement=f"measurement_{i % 9}",
            phenomena=f"phenomena_{i % 8}",
            architecture=f"architecture_{i % 4}",
            capability=f"capability_{i % 7}",
            workload=f"workload_{i % 5}",
            infrastructure=f"infrastructure_{i % 3}",
            objective=f"objective_{i % 6}",
            constraints=f"constraints_{i % 8}",
            problem_space=f"space_{i % 4}",
            investigation=f"investigation_{i % 3}",
            approach1=f"approach1_{i % 5}",
            approach2=f"approach2_{i % 6}",
            phenomenon=f"phenomenon_{i % 9}"
        )
        
        # Add document metadata
        article_text = f"Article {i+1:06d}: {filled_text} Context: {i % 100}, Domain: {i % 20}."
        test_corpus.append(article_text)
    
    return test_corpus


def main():
    """Main streaming optimization testing"""
    
    print("üöÄ GPU MEMORY OPTIMIZATION & STREAMING BENCHMARK")
    print("=" * 60)
    print("Testing memory-optimized streaming for massive corpus processing")
    print()
    
    # Initialize memory optimizer
    memory_optimizer = MemoryOptimizer()
    
    print("üîç MEMORY ANALYSIS:")
    profile = memory_optimizer.memory_profile
    print(f"   System RAM: {profile.total_memory_gb:.1f}GB total, {profile.available_memory_gb:.1f}GB available")
    print(f"   GPU Memory: {profile.gpu_memory_gb:.1f}GB total, {profile.gpu_available_gb:.1f}GB available")
    print(f"   Optimal batch size: {profile.optimal_batch_size:,}")
    print(f"   Streaming recommended: {profile.recommended_streaming}")
    
    # Test different corpus sizes with memory optimization
    test_sizes = [1000, 5000, 10000, 25000] if profile.available_memory_gb > 4 else [1000, 2500, 5000]
    
    all_results = {}
    
    for size in test_sizes:
        print(f"\nüìä TESTING CORPUS SIZE: {size:,} articles")
        print("=" * 50)
        
        # Create massive test corpus
        test_corpus = create_massive_test_corpus(size)
        
        # Create optimized streaming configuration
        streaming_config = memory_optimizer.create_streaming_config(size)
        
        print(f"‚öôÔ∏è  STREAMING CONFIG:")
        print(f"   Batch size: {streaming_config.batch_size:,}")
        print(f"   Buffer size: {streaming_config.buffer_size}")
        print(f"   Memory limit: {streaming_config.memory_limit_gb:.1f}GB")
        print(f"   GPU streaming: {streaming_config.use_gpu_streaming}")
        
        # Initialize streaming processor
        processor = StreamingProcessor(streaming_config)
        
        try:
            # Process corpus with streaming
            results = processor.process_streaming_corpus(test_corpus)
            
            all_results[f'streaming_size_{size}'] = results
            
            # Print performance summary
            print(f"\nüìà PERFORMANCE SUMMARY:")
            print(f"   Total texts: {results['total_texts']:,}")
            print(f"   Processing time: {results['processing_time']:.2f}s")
            print(f"   Throughput: {results['texts_per_second']:.1f} texts/sec")
            print(f"   Peak memory: {results['peak_memory_usage']:.1f}%")
            print(f"   Memory stable: {results['memory_efficiency']['memory_stable']}")
            print(f"   GPU utilized: {results['performance_metrics']['gpu_utilization']}")
            
        except Exception as e:
            print(f"‚ùå Error processing size {size}: {e}")
            continue
    
    # Save comprehensive results
    output_file = Path('./streaming_memory_optimization_results.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nüíæ Results saved to: {output_file}")
    
    # Performance scaling analysis
    print(f"\nüìä MEMORY OPTIMIZATION ANALYSIS:")
    for size in test_sizes:
        key = f'streaming_size_{size}'
        if key in all_results:
            result = all_results[key]
            throughput = result['texts_per_second']
            memory_peak = result['peak_memory_usage']
            gpu_used = result['performance_metrics']['gpu_utilization']
            
            print(f"   {size:6,} articles: {throughput:7.1f} texts/sec, "
                  f"{memory_peak:4.1f}% peak memory, GPU: {gpu_used}")
    
    # Theoretical scaling to 1M articles
    if all_results:
        best_result = max(all_results.values(), key=lambda x: x['texts_per_second'])
        best_throughput = best_result['texts_per_second']
        
        print(f"\n‚ö° SCALING PROJECTIONS:")
        print(f"   Best throughput: {best_throughput:.1f} texts/sec")
        
        million_time = 1_000_000 / best_throughput
        print(f"   1M articles: {million_time/3600:.1f} hours")
        
        ten_million_time = 10_000_000 / best_throughput
        print(f"   10M articles: {ten_million_time/3600:.1f} hours")
        
        print(f"   Memory efficiency: Constant usage with streaming")
        print(f"   GPU acceleration: {GPU_AVAILABLE} ({'enabled' if GPU_AVAILABLE else 'CPU fallback'})")
    
    return all_results


if __name__ == "__main__":
    main()