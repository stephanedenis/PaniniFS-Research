#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ImplÃ©mentation Phase 1 : Extension Multi-Scripts pour Corpus Multilingue
Solution immÃ©diate aux gaps identifiÃ©s (coverage 0% â†’ 30-50%)
"""

import json
import os
import unicodedata
from collections import defaultdict, Counter

class MultilingualDhatuAnalyzer:
    """Analyseur dhÄtu multilingue avec support scripts non-latins"""
    
    def __init__(self):
        self.dhatu_multilingual = {
            'EXIST': {
                'latin': ['is', 'am', 'are', 'Ãªtre', 'est', 'sein', 'ist', 'ser', 'estar', 'zijn', 'on', 'sono'],
                'arabic': ['ÙŠÙƒÙˆÙ†', 'ÙƒØ§Ù†', 'Ù‡Ùˆ', 'Ù‡ÙŠ', 'ÙŠÙˆØ¬Ø¯', 'ÙÙŠ', 'ÙƒØ§Ø¦Ù†'],
                'chinese': ['æ˜¯', 'æœ‰', 'åœ¨', 'å­˜åœ¨', 'ä¸º', 'ä¹ƒ'],
                'devanagari': ['à¤¹à¥ˆ', 'à¤¹à¥ˆà¤‚', 'à¤¥à¤¾', 'à¤¹à¥‹à¤¨à¤¾', 'à¤…à¤¸à¥à¤¤à¤¿'],
                'hebrew': ['×”×•×', '×”×™×', '×™×©', '×§×™×™×', '× ××¦×'],
                'japanese': ['ã ', 'ã§ã™', 'ã‚ã‚‹', 'ã„ã‚‹', 'ã§ã‚ã‚‹'],
                'korean': ['ì´ë‹¤', 'ìˆë‹¤', 'ì¡´ì¬í•˜ë‹¤', 'ë˜ë‹¤']
            },
            'RELATE': {
                'latin': ['in', 'on', 'at', 'dans', 'sur', 'avec', 'in', 'auf', 'an', 'en', 'sobre', 'con', 'di', 'su'],
                'arabic': ['ÙÙŠ', 'Ø¹Ù„Ù‰', 'Ø¹Ù†Ø¯', 'Ù…Ø¹', 'Ø¥Ù„Ù‰', 'Ù…Ù†', 'Ø¨ÙŠÙ†'],
                'chinese': ['åœ¨', 'ä¸Š', 'é‡Œ', 'ä¸­', 'çš„', 'ä¸', 'å’Œ'],
                'devanagari': ['à¤®à¥‡à¤‚', 'à¤ªà¤°', 'à¤•à¥‡ à¤¸à¤¾à¤¥', 'à¤¸à¥‡', 'à¤•à¥‹'],
                'hebrew': ['×‘', '×¢×œ', '×¢×', '××¦×œ', '×©×œ', '××œ'],
                'japanese': ['ã«', 'ã§', 'ã¨', 'ã‹ã‚‰', 'ã¾ã§', 'ã®'],
                'korean': ['ì—', 'ì—ì„œ', 'ì™€', 'ê³¼', 'ì˜', 'ë¡œ']
            },
            'COMM': {
                'latin': ['say', 'tell', 'talk', 'speak', 'dire', 'parler', 'sagen', 'sprechen', 'decir', 'hablar'],
                'arabic': ['Ù‚Ø§Ù„', 'ÙŠÙ‚ÙˆÙ„', 'ØªÙƒÙ„Ù…', 'Ø­Ø¯ÙŠØ«', 'ÙƒÙ„Ø§Ù…'],
                'chinese': ['è¯´', 'è®²', 'è¯', 'è¨€', 'è°ˆ'],
                'devanagari': ['à¤•à¤¹à¤¨à¤¾', 'à¤¬à¥‹à¤²à¤¨à¤¾', 'à¤¬à¤¾à¤¤', 'à¤•à¤¹'],
                'hebrew': ['×××¨', '×“×‘×¨', '××“×‘×¨', '×××¨×”'],
                'japanese': ['è¨€ã†', 'è©±ã™', 'ã„ã†', 'ã¯ãªã™'],
                'korean': ['ë§í•˜ë‹¤', 'ì´ì•¼ê¸°í•˜ë‹¤', 'ì–˜ê¸°']
            },
            'EVAL': {
                'latin': ['big', 'small', 'good', 'bad', 'grand', 'petit', 'bon', 'groÃŸ', 'klein', 'gut', 'grande'],
                'arabic': ['ÙƒØ¨ÙŠØ±', 'ØµØºÙŠØ±', 'Ø¬ÙŠØ¯', 'Ø³ÙŠØ¡', 'Ø£ÙƒØ¨Ø±'],
                'chinese': ['å¤§', 'å°', 'å¥½', 'å', 'å¾ˆ'],
                'devanagari': ['à¤¬à¤¡à¤¼à¤¾', 'à¤›à¥‹à¤Ÿà¤¾', 'à¤…à¤šà¥à¤›à¤¾', 'à¤¬à¥à¤°à¤¾'],
                'hebrew': ['×’×“×•×œ', '×§×˜×Ÿ', '×˜×•×‘', '×¨×¢'],
                'japanese': ['å¤§ãã„', 'å°ã•ã„', 'ã„ã„', 'æ‚ªã„'],
                'korean': ['í¬ë‹¤', 'ì‘ë‹¤', 'ì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤']
            },
            'ITER': {
                'latin': ['more', 'again', 'encore', 'plus', 'wieder', 'mehr', 'mÃ¡s', 'otra vez', 'ancora'],
                'arabic': ['Ø£ÙƒØ«Ø±', 'Ù…Ø±Ø©', 'Ø«Ø§Ù†ÙŠØ©', 'Ø²ÙŠØ§Ø¯Ø©'],
                'chinese': ['æ›´', 'å†', 'åˆ', 'å¤š'],
                'devanagari': ['à¤«à¤¿à¤°', 'à¤”à¤°', 'à¤…à¤§à¤¿à¤•'],
                'hebrew': ['×¢×•×“', '×©×•×‘', '×™×•×ª×¨'],
                'japanese': ['ã¾ãŸ', 'ã‚‚ã†', 'ã‚‚ã£ã¨'],
                'korean': ['ë”', 'ë‹¤ì‹œ', 'ë˜']
            },
            'MODAL': {
                'latin': ['can', 'must', 'may', 'should', 'peut', 'doit', 'kann', 'muss', 'puede', 'debe'],
                'arabic': ['ÙŠÙ…ÙƒÙ†', 'ÙŠØ¬Ø¨', 'Ù…Ù…ÙƒÙ†', 'Ù„Ø§Ø²Ù…'],
                'chinese': ['èƒ½', 'å¯ä»¥', 'åº”è¯¥', 'å¿…é¡»'],
                'devanagari': ['à¤¸à¤•à¤¨à¤¾', 'à¤šà¤¾à¤¹à¤¿à¤', 'à¤¹à¥‹à¤—à¤¾'],
                'hebrew': ['×™×›×•×œ', '×¦×¨×™×š', '××¤×©×¨'],
                'japanese': ['ã§ãã‚‹', 'ã¹ã', 'ã‹ã‚‚ã—ã‚Œãªã„'],
                'korean': ['í•  ìˆ˜ ìˆë‹¤', 'í•´ì•¼ í•˜ë‹¤', 'ì•„ë§ˆ']
            },
            'CAUSE': {
                'latin': ['make', 'do', 'cause', 'faire', 'machen', 'tun', 'hacer', 'causa'],
                'arabic': ['ÙŠÙØ¹Ù„', 'Ø³Ø¨Ø¨', 'Ø¹Ù…Ù„', 'ØµÙ†Ø¹'],
                'chinese': ['åš', 'è®©', 'ä½¿', 'é€ æˆ'],
                'devanagari': ['à¤•à¤°à¤¨à¤¾', 'à¤¬à¤¨à¤¾à¤¨à¤¾', 'à¤•à¤¾à¤°à¤£'],
                'hebrew': ['×¢×•×©×”', '×’×•×¨×', '×¢×©×”'],
                'japanese': ['ã™ã‚‹', 'ã•ã›ã‚‹', 'ä½œã‚‹'],
                'korean': ['í•˜ë‹¤', 'ë§Œë“¤ë‹¤', 'ì‹œí‚¤ë‹¤']
            },
            'FLOW': {
                'latin': ['go', 'come', 'move', 'aller', 'venir', 'gehen', 'kommen', 'ir', 'venir', 'andare'],
                'arabic': ['ÙŠØ°Ù‡Ø¨', 'ÙŠØ£ØªÙŠ', 'Ø­Ø±ÙƒØ©', 'Ø°Ù‡Ø§Ø¨'],
                'chinese': ['å»', 'æ¥', 'èµ°', 'ç§»åŠ¨'],
                'devanagari': ['à¤œà¤¾à¤¨à¤¾', 'à¤†à¤¨à¤¾', 'à¤šà¤²à¤¨à¤¾'],
                'hebrew': ['×”×•×œ×š', '×‘×', '×–×–'],
                'japanese': ['è¡Œã', 'æ¥ã‚‹', 'å‹•ã'],
                'korean': ['ê°€ë‹¤', 'ì˜¤ë‹¤', 'ì›€ì§ì´ë‹¤']
            },
            'DECIDE': {
                'latin': ['choose', 'pick', 'want', 'decide', 'choisir', 'vouloir', 'wÃ¤hlen', 'wollen', 'elegir'],
                'arabic': ['ÙŠØ®ØªØ§Ø±', 'ÙŠØ±ÙŠØ¯', 'Ù‚Ø±Ø§Ø±', 'Ø§Ø®ØªÙŠØ§Ø±'],
                'chinese': ['é€‰æ‹©', 'è¦', 'å†³å®š', 'æƒ³'],
                'devanagari': ['à¤šà¥à¤¨à¤¨à¤¾', 'à¤šà¤¾à¤¹à¤¨à¤¾', 'à¤«à¥ˆà¤¸à¤²à¤¾'],
                'hebrew': ['×‘×•×—×¨', '×¨×•×¦×”', '×”×—×œ×™×˜'],
                'japanese': ['é¸ã¶', 'ã»ã—ã„', 'æ±ºã‚ã‚‹'],
                'korean': ['ì„ íƒí•˜ë‹¤', 'ì›í•˜ë‹¤', 'ê²°ì •í•˜ë‹¤']
            }
        }
        
        # Mapping des scripts Unicode vers nos catÃ©gories
        self.script_mapping = {
            'ARABIC': 'arabic',
            'HAN': 'chinese',
            'HIRAGANA': 'japanese',
            'KATAKANA': 'japanese',
            'DEVANAGARI': 'devanagari',
            'HEBREW': 'hebrew',
            'HANGUL': 'korean',
            'LATIN': 'latin'
        }
    
    def detect_script(self, text):
        """DÃ©tecte le script principal du texte"""
        scripts = defaultdict(int)
        for char in text:
            if char.isalpha():
                try:
                    script = unicodedata.name(char).split()[0]
                    scripts[script] += 1
                except ValueError:
                    continue
        
        if not scripts:
            return 'latin'
        
        main_script = max(scripts.items(), key=lambda x: x[1])[0]
        return self.script_mapping.get(main_script, 'latin')
    
    def get_dhatu_keywords(self, dhatu, script):
        """Retourne keywords appropriÃ©s selon script dÃ©tectÃ©"""
        return self.dhatu_multilingual.get(dhatu, {}).get(script, [])
    
    def analyze_sentence_multilingual(self, sentence, language_hint=None):
        """Analyse multilingue d'une phrase"""
        # DÃ©tecter script automatiquement ou utiliser hint
        if language_hint:
            script = self.get_script_from_language(language_hint)
        else:
            script = self.detect_script(sentence)
        
        sentence_lower = sentence.lower()
        dhatu_counts = Counter()
        
        # Pour chaque dhÄtu, chercher ses keywords dans le script appropriÃ©
        for dhatu in self.dhatu_multilingual.keys():
            keywords = self.get_dhatu_keywords(dhatu, script)
            for keyword in keywords:
                if keyword in sentence_lower:
                    dhatu_counts[dhatu] += 1
        
        return {
            'detected_script': script,
            'dhatu_usage': dict(dhatu_counts),
            'total_dhatu': sum(dhatu_counts.values()),
            'coverage': len(dhatu_counts) / 9  # 9 dhÄtu total
        }
    
    def get_script_from_language(self, lang_code):
        """Mapping codes langue vers scripts"""
        lang_script_map = {
            'arb': 'arabic', 'ar': 'arabic',
            'cmn': 'chinese', 'zh': 'chinese',
            'heb': 'hebrew', 'he': 'hebrew',
            'hin': 'devanagari', 'hi': 'devanagari',
            'jpn': 'japanese', 'ja': 'japanese',
            'kor': 'korean', 'ko': 'korean'
        }
        return lang_script_map.get(lang_code, 'latin')

def test_multilingual_improvements():
    """Test des amÃ©liorations sur phrases problÃ©matiques"""
    
    analyzer = MultilingualDhatuAnalyzer()
    
    test_cases = [
        # Cas qui Ã©chouaient avant (0% coverage)
        {'text': 'Ø§Ù„Ù‚Ø·Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØª', 'lang': 'arb', 'description': 'Arabe: Chat dans maison'},
        {'text': 'çŒ«åœ¨æˆ¿å­é‡Œ', 'lang': 'cmn', 'description': 'Chinois: Chat dans maison'},
        {'text': '×”×—×ª×•×œ ×‘×‘×™×ª', 'lang': 'heb', 'description': 'HÃ©breu: Chat dans maison'},
        {'text': 'à¤¬à¤¿à¤²à¥à¤²à¥€ à¤˜à¤° à¤®à¥‡à¤‚ à¤¹à¥ˆ', 'lang': 'hin', 'description': 'Hindi: Chat dans maison'},
        {'text': 'çŒ«ã¯å®¶ã«ã„ã¾ã™', 'lang': 'jpn', 'description': 'Japonais: Chat dans maison'},
        {'text': 'ê³ ì–‘ì´ëŠ” ì§‘ì— ìˆë‹¤', 'lang': 'kor', 'description': 'CorÃ©en: Chat dans maison'},
        
        # Cas de contrÃ´le (dÃ©jÃ  bons)
        {'text': 'The cat is in the house', 'lang': 'en', 'description': 'Anglais: Chat dans maison'},
        {'text': 'Le chat est dans la maison', 'lang': 'fr', 'description': 'FranÃ§ais: Chat dans maison'}
    ]
    
    print("ğŸ§ª TEST AMÃ‰LIORATIONS MULTILINGUES")
    print("=" * 50)
    
    for case in test_cases:
        result = analyzer.analyze_sentence_multilingual(case['text'], case['lang'])
        
        print(f"\nğŸ“ {case['description']}")
        print(f"   Texte: {case['text']}")
        print(f"   Script dÃ©tectÃ©: {result['detected_script']}")
        print(f"   Coverage: {result['coverage']:.1%}")
        print(f"   DhÄtu dÃ©tectÃ©s: {result['dhatu_usage']}")
        print(f"   Total dhÄtu: {result['total_dhatu']}")

def update_existing_corpus_analysis():
    """Met Ã  jour l'analyse du corpus avec le nouveau systÃ¨me"""
    
    analyzer = MultilingualDhatuAnalyzer()
    experiments_path = "/home/stephane/GitHub/PaniniFS-Research/experiments/dhatu"
    
    # Langues Ã  tester
    languages = ['arb', 'cmn', 'heb', 'hin', 'jpn', 'kor']
    
    results = {}
    
    print("\nğŸ”„ MISE Ã€ JOUR ANALYSE CORPUS")
    print("=" * 40)
    
    for lang in languages:
        # Charger donnÃ©es existantes
        file_path = os.path.join(experiments_path, "prompts_child", f"{lang}.json")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if 'items' not in data:
                print(f"âŒ {lang}: Pas de donnÃ©es 'items'")
                continue
            
            total_sentences = len(data['items'])
            total_coverage = 0
            global_dhatu = Counter()
            
            # Analyser chaque phrase avec nouveau systÃ¨me
            for item in data['items']:
                sentence = item.get('text', '')
                if sentence:
                    result = analyzer.analyze_sentence_multilingual(sentence, lang)
                    total_coverage += result['coverage']
                    
                    for dhatu, count in result['dhatu_usage'].items():
                        global_dhatu[dhatu] += count
            
            avg_coverage = total_coverage / total_sentences if total_sentences > 0 else 0
            
            results[lang] = {
                'avg_coverage': avg_coverage,
                'total_sentences': total_sentences,
                'dhatu_usage': dict(global_dhatu),
                'improvement': avg_coverage  # ComparÃ© Ã  0% avant
            }
            
            print(f"âœ… {lang.upper()}: {avg_coverage:.1%} coverage ({total_sentences} phrases)")
            if global_dhatu:
                top_dhatu = global_dhatu.most_common(3)
                print(f"   Top dhÄtu: {top_dhatu}")
        
        except FileNotFoundError:
            print(f"âŒ {lang}: Fichier non trouvÃ©")
        except Exception as e:
            print(f"âŒ {lang}: Erreur {e}")
    
    return results

def generate_improvement_report(results):
    """GÃ©nÃ¨re rapport d'amÃ©lioration"""
    
    report_content = f"""# ğŸ“ˆ RAPPORT D'AMÃ‰LIORATION : Extension Multilingue Phase 1

*RÃ©sultats aprÃ¨s implÃ©mentation dÃ©tecteur multi-scripts*

## ğŸ¯ **AmÃ©liorations MesurÃ©es**

### **Avant vs AprÃ¨s (Coverage %)**

| Langue | Famille | Avant | AprÃ¨s | AmÃ©lioration | Status |
|--------|---------|-------|-------|--------------|--------|
"""
    
    for lang, data in results.items():
        lang_names = {
            'arb': 'Arabe',
            'cmn': 'Chinois', 
            'heb': 'HÃ©breu',
            'hin': 'Hindi',
            'jpn': 'Japonais',
            'kor': 'CorÃ©en'
        }
        
        families = {
            'arb': 'Afro-Asiatique',
            'cmn': 'Sino-TibÃ©taine',
            'heb': 'Afro-Asiatique', 
            'hin': 'Indo-EuropÃ©enne',
            'jpn': 'AltaÃ¯que',
            'kor': 'AltaÃ¯que'
        }
        
        lang_name = lang_names.get(lang, lang)
        family = families.get(lang, 'Autre')
        before = 0.0
        after = data['avg_coverage']
        improvement = after - before
        
        status = "ğŸš€ SuccÃ¨s" if after > 0.2 else "âš ï¸ Partiel" if after > 0.05 else "âŒ Ã‰chec"
        
        report_content += f"| {lang_name} | {family} | {before:.1%} | {after:.1%} | +{improvement:.1%} | {status} |\n"
    
    # Calculs globaux
    total_improvement = sum(r['avg_coverage'] for r in results.values()) / len(results)
    
    report_content += f"""
## ğŸ“Š **Statistiques Globales**

- **Coverage moyenne** : 0.0% â†’ {total_improvement:.1%}
- **Langues amÃ©liorÃ©es** : {sum(1 for r in results.values() if r['avg_coverage'] > 0.05)}/{len(results)}
- **Meilleure performance** : {max(results.items(), key=lambda x: x[1]['avg_coverage'])[0].upper()} ({max(r['avg_coverage'] for r in results.values()):.1%})

## ğŸ§¬ **DhÄtu les Plus DÃ©tectÃ©s**

"""
    
    # AgrÃ©ger usage dhÄtu global
    global_dhatu_usage = Counter()
    for data in results.values():
        for dhatu, count in data['dhatu_usage'].items():
            global_dhatu_usage[dhatu] += count
    
    for dhatu, count in global_dhatu_usage.most_common():
        lang_count = sum(1 for r in results.values() if dhatu in r['dhatu_usage'])
        universality = lang_count / len(results) * 100
        report_content += f"- **{dhatu}** : {count} occurrences, {lang_count}/{len(results)} langues ({universality:.0f}%)\n"
    
    report_content += f"""
## ğŸ¯ **Ã‰valuation Phase 1**

### **âœ… SuccÃ¨s**
- DÃ©tection automatique scripts Unicode
- Keywords multilingues opÃ©rationnels
- AmÃ©lioration mesurable langues problÃ©matiques

### **âš ï¸ Limitations Persistantes**
- Algorithme keywords simple (pas d'analyse morphologique)
- Couverture encore faible (<30% pour la plupart)
- Manque validation linguistes natifs

### **ğŸš€ Prochaines Ã‰tapes**
1. **Phase 2** : Analyseur morphologique (racines sÃ©mitiques, agglutination)
2. **Validation humaine** : Ã‰chantillons contrÃ´lÃ©s par linguistes
3. **Corpus Ã©tendus** : Plus de phrases par langue

---

*Rapport gÃ©nÃ©rÃ© automatiquement - {__import__('datetime').datetime.now().strftime('%d/%m/%Y %H:%M')}*
"""
    
    return report_content

def main():
    """Fonction principale de test"""
    
    print("ğŸš€ DÃ‰MARRAGE SOLUTION PHASE 1 : Extension Multilingue")
    print("=" * 60)
    
    # Test amÃ©liorations
    test_multilingual_improvements()
    
    # Analyse corpus mis Ã  jour
    results = update_existing_corpus_analysis()
    
    # GÃ©nÃ©rer rapport
    if results:
        report = generate_improvement_report(results)
        
        # Sauvegarder rapport
        output_path = "/home/stephane/GitHub/PaniniFS-Research/data/references_cache/RAPPORT_AMELIORATION_PHASE1.md"
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nğŸ“„ Rapport d'amÃ©lioration gÃ©nÃ©rÃ©: {output_path}")
    
    print("\nâœ… Solution Phase 1 terminÃ©e!")

if __name__ == "__main__":
    main()
