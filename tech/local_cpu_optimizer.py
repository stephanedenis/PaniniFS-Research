#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CPU Local Optimizer - 16 Cores Xeon E5-2650
Optimiseur imm√©diat pour traitement dhƒÅtu sur votre machine actuelle
"""

import multiprocessing as mp
import numpy as np
import psutil
import time
import json
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
import gc


@dataclass
class LocalProcessingConfig:
    """Configuration optimis√©e pour Xeon E5-2650 16 cores"""
    worker_processes: int = 14  # 16 cores - 2 pour syst√®me
    thread_pool_size: int = 32
    batch_size: int = 5000
    memory_limit_gb: int = 35
    numpy_threads: int = 16
    gc_frequency: int = 1000


class LocalCPUOptimizer:
    """Optimiseur CPU pour performance maximale locale"""
    
    def __init__(self, config: LocalProcessingConfig = None):
        self.config = config or LocalProcessingConfig()
        self._setup_environment()
        
        # Stats de performance
        self.processing_stats = {
            'texts_processed': 0,
            'processing_time': 0.0,
            'memory_peak': 0.0,
            'cpu_utilization': []
        }
    
    def _setup_environment(self):
        """Configuration environnement optimis√©"""
        # Configuration NumPy pour 16 cores
        import os
        os.environ['OMP_NUM_THREADS'] = str(self.config.numpy_threads)
        os.environ['OPENBLAS_NUM_THREADS'] = str(self.config.numpy_threads)
        os.environ['MKL_NUM_THREADS'] = str(self.config.numpy_threads)
        os.environ['NUMEXPR_NUM_THREADS'] = str(self.config.numpy_threads)
        
        print(f"üîß Configuration CPU optimis√©e:")
        print(f"   ‚Ä¢ Worker processes: {self.config.worker_processes}")
        print(f"   ‚Ä¢ Thread pool: {self.config.thread_pool_size}")
        print(f"   ‚Ä¢ NumPy threads: {self.config.numpy_threads}")
        print(f"   ‚Ä¢ Batch size: {self.config.batch_size}")
        print(f"   ‚Ä¢ Memory limit: {self.config.memory_limit_gb}GB")
    
    def optimize_dhatu_vectorization(self, texts: List[str]) -> np.ndarray:
        """Vectorisation dhƒÅtu optimis√©e multi-core"""
        
        # DhƒÅtus universels pour traitement vectoris√©
        dhatu_patterns = {
            'RELATE': np.array([2, 3, 5]),
            'MODAL': np.array([2, 3, 7]),
            'EXIST': np.array([2, 5, 7]),
            'EVAL': np.array([3, 5, 7]),
            'COMM': np.array([2, 3, 11]),
            'CAUSE': np.array([2, 5, 11]),
            'ITER': np.array([3, 5, 11]),
            'DECIDE': np.array([2, 7, 11]),
            'FEEL': np.array([3, 7, 11])
        }
        
        # Preprocessing vectoris√©
        def process_batch_vectorized(text_batch):
            """Traitement vectoris√© d'un batch de textes"""
            batch_size = len(text_batch)
            vectors = np.zeros((batch_size, 9))  # 9 dhƒÅtus
            
            # Vectorisation simultan√©e de tous les dhƒÅtus
            for i, (dhatu_name, pattern) in enumerate(dhatu_patterns.items()):
                # Recherche pattern dans tous les textes du batch
                for j, text in enumerate(text_batch):
                    # Calcul score dhƒÅtu optimis√©
                    words = text.lower().split()
                    word_count = len(words)
                    
                    if word_count > 0:
                        # Score bas√© sur pattern prime
                        base_score = np.prod(pattern) / (word_count + 1)
                        vectors[j, i] = base_score
            
            return vectors
        
        # Traitement par chunks avec multiprocessing
        chunks = [texts[i:i + self.config.batch_size] 
                 for i in range(0, len(texts), self.config.batch_size)]
        
        print(f"üöÄ Traitement vectoris√©: {len(chunks)} batches de {self.config.batch_size}")
        
        start_time = time.time()
        
        with ProcessPoolExecutor(max_workers=self.config.worker_processes) as executor:
            results = list(executor.map(process_batch_vectorized, chunks))
        
        # Concatenation des r√©sultats
        if results:
            final_vectors = np.vstack(results)
        else:
            final_vectors = np.array([])
        
        processing_time = time.time() - start_time
        throughput = len(texts) / processing_time if processing_time > 0 else 0
        
        print(f"‚úÖ Vectorisation termin√©e: {throughput:.0f} textes/sec")
        
        return final_vectors
    
    def process_corpus_optimized(self, corpus_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Traitement corpus optimis√© avec monitoring"""
        
        print(f"üìä TRAITEMENT CORPUS OPTIMIS√â")
        print(f"   Corpus size: {len(corpus_data)} articles")
        print(f"   Configuration: {self.config.worker_processes} workers")
        
        start_time = time.time()
        memory_start = psutil.Process().memory_info().rss / 1024**3
        
        # Extraction textes pour vectorisation
        texts = []
        metadata = []
        
        for i, article in enumerate(corpus_data):
            if isinstance(article, dict):
                text = article.get('content', '') or article.get('text', '') or str(article)
            else:
                text = str(article)
            
            texts.append(text)
            metadata.append({
                'index': i,
                'length': len(text),
                'word_count': len(text.split())
            })
            
            # Garbage collection p√©riodique
            if i % self.config.gc_frequency == 0 and i > 0:
                gc.collect()
                current_memory = psutil.Process().memory_info().rss / 1024**3
                if current_memory > self.config.memory_limit_gb:
                    print(f"‚ö†Ô∏è Memory limit reached: {current_memory:.1f}GB")
                    break
        
        print(f"üìù Textes extraits: {len(texts)}")
        
        # Vectorisation dhƒÅtu optimis√©e
        dhatu_vectors = self.optimize_dhatu_vectorization(texts)
        
        # Analyse statistique rapide
        processing_time = time.time() - start_time
        memory_peak = psutil.Process().memory_info().rss / 1024**3
        throughput = len(texts) / processing_time
        
        # Statistiques dhƒÅtu
        dhatu_stats = {}
        if dhatu_vectors.size > 0:
            dhatu_names = ['RELATE', 'MODAL', 'EXIST', 'EVAL', 'COMM', 
                          'CAUSE', 'ITER', 'DECIDE', 'FEEL']
            
            for i, dhatu in enumerate(dhatu_names):
                column = dhatu_vectors[:, i]
                dhatu_stats[dhatu] = {
                    'mean': float(np.mean(column)),
                    'std': float(np.std(column)),
                    'min': float(np.min(column)),
                    'max': float(np.max(column)),
                    'non_zero_count': int(np.count_nonzero(column))
                }
        
        # Mise √† jour stats globales
        self.processing_stats.update({
            'texts_processed': len(texts),
            'processing_time': processing_time,
            'memory_peak': memory_peak,
            'throughput': throughput
        })
        
        results = {
            'corpus_size': len(texts),
            'dhatu_vectors': dhatu_vectors.tolist() if dhatu_vectors.size > 0 else [],
            'dhatu_statistics': dhatu_stats,
            'metadata': metadata,
            'performance': {
                'processing_time_seconds': processing_time,
                'throughput_texts_per_second': throughput,
                'memory_peak_gb': memory_peak,
                'memory_start_gb': memory_start,
                'cpu_cores_used': self.config.worker_processes
            }
        }
        
        print(f"‚úÖ TRAITEMENT TERMIN√â:")
        print(f"   ‚Ä¢ Temps: {processing_time:.2f}s")
        print(f"   ‚Ä¢ Throughput: {throughput:.0f} textes/sec")
        print(f"   ‚Ä¢ M√©moire pic: {memory_peak:.1f}GB")
        print(f"   ‚Ä¢ Efficacit√©: {throughput/self.config.worker_processes:.0f} textes/sec/core")
        
        return results
    
    def benchmark_local_performance(self, test_sizes: List[int] = None) -> Dict[str, Any]:
        """Benchmark performance locale avec diff√©rentes tailles"""
        
        if test_sizes is None:
            test_sizes = [100, 500, 1000, 5000, 10000]
        
        print(f"üèÅ BENCHMARK PERFORMANCE LOCALE")
        print(f"   Test sizes: {test_sizes}")
        
        benchmark_results = {}
        
        for size in test_sizes:
            print(f"\nüìä Test avec {size} textes...")
            
            # G√©n√©ration corpus test
            test_corpus = []
            for i in range(size):
                test_text = f"Article {i} with dhƒÅtu patterns: relate connect modal exist evaluate communicate cause iterate decide feel emotional linguistic semantic processing analysis research development validation testing optimization performance scaling distributed cluster computing vectorization machine learning artificial intelligence natural language understanding comprehension generation transformation translation interpretation analysis synthesis."
                test_corpus.append({'content': test_text, 'id': i})
            
            # Test performance
            start_time = time.time()
            cpu_before = psutil.cpu_percent(interval=None)
            memory_before = psutil.Process().memory_info().rss / 1024**3
            
            results = self.process_corpus_optimized(test_corpus)
            
            end_time = time.time()
            cpu_after = psutil.cpu_percent(interval=1)
            memory_after = psutil.Process().memory_info().rss / 1024**3
            
            # Calcul m√©triques
            processing_time = end_time - start_time
            throughput = size / processing_time
            memory_used = memory_after - memory_before
            cpu_usage = (cpu_before + cpu_after) / 2
            
            benchmark_results[f"size_{size}"] = {
                'corpus_size': size,
                'processing_time': processing_time,
                'throughput': throughput,
                'memory_used_gb': memory_used,
                'cpu_usage_percent': cpu_usage,
                'efficiency_per_core': throughput / self.config.worker_processes,
                'dhatu_vectors_generated': len(results.get('dhatu_vectors', [])),
                'scaling_factor': throughput / (test_sizes[0] / benchmark_results.get(f"size_{test_sizes[0]}", {}).get('processing_time', 1)) if test_sizes[0] in [s for s in test_sizes if f"size_{s}" in benchmark_results] else 1.0
            }
            
            print(f"   ‚Üí {throughput:.0f} textes/sec, {memory_used:.2f}GB RAM")
            
            # Cleanup entre tests
            gc.collect()
            time.sleep(1)  # Pause pour stabilisation
        
        # Analyse scaling
        if len(benchmark_results) > 1:
            sizes = sorted([int(k.split('_')[1]) for k in benchmark_results.keys()])
            throughputs = [benchmark_results[f"size_{s}"]['throughput'] for s in sizes]
            
            scaling_efficiency = []
            for i in range(1, len(throughputs)):
                expected = throughputs[0] * (sizes[i] / sizes[0])
                actual = throughputs[i]
                efficiency = (actual / expected) * 100 if expected > 0 else 0
                scaling_efficiency.append(efficiency)
            
            avg_scaling_efficiency = np.mean(scaling_efficiency) if scaling_efficiency else 100
        else:
            avg_scaling_efficiency = 100
        
        summary = {
            'benchmark_results': benchmark_results,
            'scaling_analysis': {
                'average_efficiency': avg_scaling_efficiency,
                'best_throughput': max([r['throughput'] for r in benchmark_results.values()]),
                'optimal_batch_size': max(benchmark_results.keys(), key=lambda k: benchmark_results[k]['throughput']),
                'memory_scaling': 'linear' if avg_scaling_efficiency > 80 else 'sublinear'
            },
            'system_specs': {
                'cpu_cores': self.config.worker_processes,
                'memory_available_gb': psutil.virtual_memory().available / 1024**3,
                'cpu_model': 'Xeon E5-2650',
                'optimization_level': 'HIGH'
            }
        }
        
        print(f"\nüìà R√âSUM√â BENCHMARK:")
        print(f"   ‚Ä¢ Meilleur throughput: {summary['scaling_analysis']['best_throughput']:.0f} textes/sec")
        print(f"   ‚Ä¢ Efficacit√© scaling: {avg_scaling_efficiency:.1f}%")
        print(f"   ‚Ä¢ Taille optimale: {summary['scaling_analysis']['optimal_batch_size']}")
        
        return summary


def create_test_corpus(size: int = 1000) -> List[Dict[str, Any]]:
    """G√©n√®re corpus de test pour validation"""
    
    dhatu_examples = {
        'RELATE': 'connect link associate relate bond join unite merge combine integrate',
        'MODAL': 'can could should would might may must shall will modal possibility',
        'EXIST': 'be is are was were been being exist presence state reality truth',
        'EVAL': 'good bad better worse best worst evaluate assess judge analyze rating',
        'COMM': 'say tell speak talk communicate express convey share inform message',
        'CAUSE': 'because since due make create cause effect reason result consequence',
        'ITER': 'again repeat iterate loop cycle continue process step sequential order',
        'DECIDE': 'choose select decide determine conclude resolve judgment decision choice',
        'FEEL': 'feel emotion happy sad angry joy love hate fear hope sentiment'
    }
    
    corpus = []
    for i in range(size):
        # M√©lange des dhƒÅtus pour variabilit√©
        selected_dhatus = np.random.choice(list(dhatu_examples.keys()), 
                                         size=np.random.randint(2, 6), 
                                         replace=False)
        
        text_parts = []
        for dhatu in selected_dhatus:
            words = dhatu_examples[dhatu].split()
            selected_words = np.random.choice(words, size=np.random.randint(2, 5), replace=False)
            text_parts.extend(selected_words)
        
        # Construction texte
        text = f"Article {i}: " + " ".join(text_parts) + f" research analysis development validation testing optimization performance scaling distributed computing."
        
        corpus.append({
            'id': i,
            'content': text,
            'dhatus_present': list(selected_dhatus),
            'word_count': len(text.split()),
            'source': 'generated_test'
        })
    
    return corpus


def main():
    """Test complet optimiseur CPU local"""
    
    print("üíª OPTIMISEUR CPU LOCAL - XEON E5-2650")
    print("=" * 50)
    print("Test performance maximale sur votre machine actuelle")
    print()
    
    # Configuration syst√®me
    config = LocalProcessingConfig()
    optimizer = LocalCPUOptimizer(config)
    
    print(f"üñ•Ô∏è CONFIGURATION SYST√àME:")
    print(f"   ‚Ä¢ CPU: {psutil.cpu_count()} cores ({psutil.cpu_count(logical=False)} physical)")
    print(f"   ‚Ä¢ RAM: {psutil.virtual_memory().total / 1024**3:.1f}GB total")
    print(f"   ‚Ä¢ RAM libre: {psutil.virtual_memory().available / 1024**3:.1f}GB")
    
    # Test rapide
    print(f"\nüöÄ TEST RAPIDE - 1000 articles")
    test_corpus = create_test_corpus(1000)
    
    start_time = time.time()
    results = optimizer.process_corpus_optimized(test_corpus)
    total_time = time.time() - start_time
    
    throughput = 1000 / total_time
    print(f"\n‚ö° R√âSULTAT TEST RAPIDE:")
    print(f"   ‚Ä¢ Throughput: {throughput:.0f} textes/sec")
    print(f"   ‚Ä¢ Efficacit√©: {throughput/config.worker_processes:.0f} textes/sec/core")
    print(f"   ‚Ä¢ Extrapolation 100k: {100000/throughput:.1f} secondes")
    
    # Benchmark complet
    print(f"\nüìä BENCHMARK COMPLET")
    benchmark = optimizer.benchmark_local_performance([500, 1000, 2000, 5000])
    
    # Sauvegarde r√©sultats
    output_data = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'system_info': {
            'cpu_model': 'Intel Xeon E5-2650',
            'cpu_cores': psutil.cpu_count(),
            'ram_gb': psutil.virtual_memory().total / 1024**3,
            'optimization_config': {
                'worker_processes': config.worker_processes,
                'batch_size': config.batch_size,
                'memory_limit_gb': config.memory_limit_gb
            }
        },
        'quick_test_results': {
            'corpus_size': 1000,
            'processing_time': total_time,
            'throughput': throughput,
            'dhatu_analysis': results.get('dhatu_statistics', {})
        },
        'benchmark_results': benchmark
    }
    
    with open('local_cpu_optimization_results.json', 'w') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nüíæ R√©sultats sauvegard√©s: local_cpu_optimization_results.json")
    
    # Recommandations
    best_throughput = benchmark['scaling_analysis']['best_throughput']
    
    print(f"\nüéØ RECOMMANDATIONS:")
    if best_throughput > 5000:
        print(f"   ‚úÖ Performance excellente: {best_throughput:.0f} textes/sec")
        print(f"   ‚Üí Pr√™t pour corpus moyens (10k-50k articles) en local")
    elif best_throughput > 2000:
        print(f"   ‚úÖ Performance bonne: {best_throughput:.0f} textes/sec")
        print(f"   ‚Üí Convenable pour d√©veloppement et tests")
    else:
        print(f"   ‚ö†Ô∏è Performance limit√©e: {best_throughput:.0f} textes/sec")
        print(f"   ‚Üí Focus sur Colab pour gros corpus")
    
    print(f"\nüöÄ PROCHAINES √âTAPES:")
    print(f"   1. Optimisation actuelle: CPU 16 cores = {best_throughput:.0f} textes/sec")
    print(f"   2. Avec RX 480 (150‚Ç¨): Estimation 10-20x = {best_throughput*15:.0f} textes/sec")
    print(f"   3. Workflow hybride: Local + Colab = illimit√©")
    
    return output_data


if __name__ == "__main__":
    main()