#!/usr/bin/env python3
"""
ğŸ’¾ BACKUP MANAGER - Gestionnaire de Sauvegardes
Utilitaire pour crÃ©er et gÃ©rer les sauvegardes du projet
"""

import os
import shutil
import json
from pathlib import Path
from datetime import datetime
import tarfile

def create_incremental_backup():
    """CrÃ©er sauvegarde incrÃ©mentale intelligente"""
    
    print("ğŸ’¾ BACKUP MANAGER")
    print("ğŸ¯ Sauvegarde incrÃ©mentale intelligente")
    print("=" * 50)
    
    # Configuration backup
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_dir = Path("archives/backups")
    backup_name = f"backup_incremental_{timestamp}"
    backup_path = backup_dir / backup_name
    
    # CrÃ©er structure backup
    backup_dir.mkdir(parents=True, exist_ok=True)
    backup_path.mkdir(exist_ok=True)
    
    print(f"ğŸ“ Destination : {backup_path}")
    
    # Ã‰lÃ©ments critiques Ã  sauvegarder
    critical_items = [
        ("copilotage", "ğŸ¤– Coordination agents"),
        ("production", "ğŸ“¦ Documents finaux"),
        ("research/discoveries", "ğŸ“š DÃ©couvertes recherche"),
        ("data/references_cache", "ğŸ“Š Cache rÃ©fÃ©rences"),
        ("README.md", "ğŸ“„ Documentation principale"),
        ("README_NAVIGATION.md", "ğŸ“‹ Guide navigation")
    ]
    
    backed_up = 0
    total_size = 0
    
    print(f"\nğŸ“¦ SAUVEGARDE EN COURS :")
    
    for item, description in critical_items:
        source = Path(item)
        if source.exists():
            try:
                dest = backup_path / item
                dest.parent.mkdir(parents=True, exist_ok=True)
                
                if source.is_dir():
                    shutil.copytree(source, dest, dirs_exist_ok=True)
                    size = sum(f.stat().st_size for f in source.rglob('*') if f.is_file())
                else:
                    shutil.copy2(source, dest)
                    size = source.stat().st_size
                
                backed_up += 1
                total_size += size
                print(f"   âœ… {item} - {description} ({size} bytes)")
                
            except Exception as e:
                print(f"   âŒ {item} - Erreur : {e}")
        else:
            print(f"   â­ï¸  {item} - N'existe pas")
    
    # MÃ©tadonnÃ©es backup
    metadata = {
        "timestamp": timestamp,
        "items_backed_up": backed_up,
        "total_size": total_size,
        "backup_type": "incremental",
        "critical_items": [item for item, _ in critical_items],
        "created_by": "backup_manager.py"
    }
    
    with open(backup_path / "backup_metadata.json", "w") as f:
        json.dump(metadata, f, indent=2)
    
    print(f"\nğŸ“Š RÃ‰SUMÃ‰ BACKUP :")
    print(f"   ğŸ“¦ Ã‰lÃ©ments sauvÃ©s : {backed_up}")
    print(f"   ğŸ’¾ Taille totale : {total_size:,} bytes")
    print(f"   ğŸ“ Localisation : {backup_path}")
    
    return backup_path, backed_up

def create_archive_backup():
    """CrÃ©er archive complÃ¨te compressÃ©e"""
    
    print(f"\nğŸ—œï¸  CRÃ‰ATION ARCHIVE COMPRESSÃ‰E :")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    archive_name = f"paniniFS_complete_{timestamp}.tar.gz"
    archive_path = Path("archives/backups") / archive_name
    
    # CrÃ©er archive tar.gz
    with tarfile.open(archive_path, "w:gz") as tar:
        # Ajouter dossiers critiques
        for item in ["copilotage", "production", "research", "data"]:
            if Path(item).exists():
                tar.add(item, arcname=item)
                print(f"   âœ… AjoutÃ© : {item}/")
        
        # Ajouter fichiers racine importants
        for item in ["README.md", "README_NAVIGATION.md"]:
            if Path(item).exists():
                tar.add(item, arcname=item)
                print(f"   âœ… AjoutÃ© : {item}")
    
    archive_size = archive_path.stat().st_size
    print(f"   ğŸ“¦ Archive crÃ©Ã©e : {archive_name}")
    print(f"   ğŸ’¾ Taille : {archive_size:,} bytes")
    
    return archive_path, archive_size

def cleanup_old_backups(keep_last=5):
    """Nettoyer anciennes sauvegardes"""
    
    print(f"\nğŸ§¹ NETTOYAGE ANCIENNES SAUVEGARDES :")
    
    backup_dir = Path("archives/backups")
    if not backup_dir.exists():
        print("   â­ï¸  Aucun dossier backup")
        return
    
    # Lister backups existants
    backup_dirs = [d for d in backup_dir.iterdir() if d.is_dir() and d.name.startswith("backup_")]
    backup_files = [f for f in backup_dir.iterdir() if f.is_file() and f.name.endswith(".tar.gz")]
    
    # Trier par date (plus rÃ©cent en premier)
    backup_dirs.sort(key=lambda x: x.name, reverse=True)
    backup_files.sort(key=lambda x: x.name, reverse=True)
    
    cleaned = 0
    
    # Nettoyer dossiers backup
    if len(backup_dirs) > keep_last:
        for old_backup in backup_dirs[keep_last:]:
            try:
                shutil.rmtree(old_backup)
                cleaned += 1
                print(f"   ğŸ—‘ï¸  SupprimÃ© : {old_backup.name}")
            except Exception as e:
                print(f"   âŒ Erreur suppression {old_backup.name} : {e}")
    
    # Nettoyer archives
    if len(backup_files) > keep_last:
        for old_archive in backup_files[keep_last:]:
            try:
                old_archive.unlink()
                cleaned += 1
                print(f"   ğŸ—‘ï¸  SupprimÃ© : {old_archive.name}")
            except Exception as e:
                print(f"   âŒ Erreur suppression {old_archive.name} : {e}")
    
    print(f"   ğŸ“Š {cleaned} anciens backups supprimÃ©s")
    print(f"   ğŸ“ {len(backup_dirs[:keep_last]) + len(backup_files[:keep_last])} backups conservÃ©s")

def list_available_backups():
    """Lister sauvegardes disponibles"""
    
    print(f"\nğŸ“‹ SAUVEGARDES DISPONIBLES :")
    
    backup_dir = Path("archives/backups")
    if not backup_dir.exists():
        print("   â­ï¸  Aucune sauvegarde trouvÃ©e")
        return
    
    # Lister dossiers backup
    backup_dirs = [d for d in backup_dir.iterdir() if d.is_dir() and d.name.startswith("backup_")]
    backup_files = [f for f in backup_dir.iterdir() if f.is_file() and f.name.endswith(".tar.gz")]
    
    for backup in sorted(backup_dirs, reverse=True):
        metadata_file = backup / "backup_metadata.json"
        if metadata_file.exists():
            with open(metadata_file) as f:
                metadata = json.load(f)
            print(f"   ğŸ“ {backup.name} - {metadata.get('items_backed_up', '?')} items")
        else:
            print(f"   ğŸ“ {backup.name} - MÃ©tadonnÃ©es manquantes")
    
    for archive in sorted(backup_files, reverse=True):
        size = archive.stat().st_size
        print(f"   ğŸ—œï¸  {archive.name} - {size:,} bytes")

def main():
    """Gestionnaire principal de sauvegardes"""
    
    os.chdir(Path(__file__).parent.parent.parent)  # Retour racine
    
    print("ğŸ’¾ BACKUP MANAGER PANINI-FS")
    print("ğŸ¯ Gestion intelligente des sauvegardes")
    print()
    
    # 1. Sauvegarde incrÃ©mentale
    backup_path, items = create_incremental_backup()
    
    # 2. Archive complÃ¨te
    archive_path, archive_size = create_archive_backup()
    
    # 3. Nettoyage automatique
    cleanup_old_backups(keep_last=3)
    
    # 4. Liste des backups
    list_available_backups()
    
    print(f"\nğŸ‰ BACKUP TERMINÃ‰")
    print(f"   ğŸ“ Backup incrÃ©mental : {items} Ã©lÃ©ments")
    print(f"   ğŸ—œï¸  Archive complÃ¨te : {archive_size:,} bytes")
    print(f"   ğŸ“ Localisation : archives/backups/")
    
    return 0

if __name__ == "__main__":
    exit(main())
