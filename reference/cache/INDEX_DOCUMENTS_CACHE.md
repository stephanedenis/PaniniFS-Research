# INDEX CACHE DOCUMENTS RECHERCHE
*Documents de référence pour l'état de l'art compression sémantique*

Date: 7 septembre 2025  
Objectif: Copie de sécurité et analyse approfondie des sources clés

## DOCUMENTS TÉLÉCHARGÉS

### 1. Compression Sémantique et Information Theory

#### **tokens_to_thoughts_2505.17117.pdf** (6.6 MB)
- **Titre**: "From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning"
- **Auteurs**: Chen Shani, Liron Soffer, Dan Jurafsky, Yann LeCun, Ravid Shwartz-Ziv
- **Date**: Mai 2025 (dernière révision Aug 2025)
- **Importance**: Framework Rate-Distortion + Information Bottleneck pour comparaison LLM/humains
- **Findings clés**: 
  - LLMs biaisés vers compression statistique agressive
  - Humains privilégient nuance adaptative
  - Trade-off compression/meaning non résolu

#### **shannon_1948_mathematical_theory_communication.pdf** (358 KB) ✅ **NOUVEAU**
- **Titre**: "A Mathematical Theory of Communication"
- **Auteur**: Claude E. Shannon
- **Date**: 1948
- **Importance**: ★★★★★ - Article fondateur théorie de l'information
- **Findings clés**:
  - Définition entropie informationnelle
  - Limites fondamentales transmission
  - Base théorique moderne compression

#### **gpt_compression_kolmogorov_2308.06942.pdf** (482 KB)  
- **Titre**: "Approximating Human-Like Few-shot Learning with GPT-based Compression"
- **Auteurs**: Cynthia Huang, Yuqing Xie, Zhiying Jiang, Jimmy Lin, Ming Li
- **Date**: Août 2023
- **Importance**: Utilisation GPT pour approximer complexité Kolmogorov
- **Findings clés**:
  - Compression ratio 15.5 sur enwik9 avec LLAMA2-7B
  - Learning conceptualisé comme compression information
  - Équivalence objective pré-training GPT ↔ compression length

#### **deletang_2024_language_modeling_is_compression.pdf** (2.2 MB) ✅ **NOUVEAU**
- **Titre**: "Language Modeling Is Compression"
- **Auteurs**: Grégoire Delétang, et al.
- **Date**: 2024
- **Importance**: ★★★★★ - Démonstration équivalence modélisation ↔ compression
- **Findings clés**:
  - Proof formelle équivalence LM/compression
  - Benchmarks compression cross-domain
  - Implications théoriques majeures

### 2. Framework Quantique et Non-Classique

#### **quantum_semantic_framework_2506.10077.pdf** (890 KB)
- **Titre**: "A quantum semantic framework for natural language processing"  
- **Auteurs**: Christopher J. Agostino, Quan Le Thien, Molly Apsel, et al.
- **Date**: Juin 2025 (révision Juillet 2025)
- **Importance**: Validation limitations classiques, approche quantique sémantique
- **Findings clés**:
  - Dégénérescence sémantique impose limitations fondamentales sur LLMs
  - Complexité Kolmogorov explode combinatoriellement 
  - Bell inequality violations en interprétation linguistique
  - Approches frequentist classiques nécessairement lossy

### 4. Intelligence Artificielle et Architectures

#### **vaswani_2017_attention_is_all_you_need.pdf** (2.1 MB) ✅ **NOUVEAU**
- **Titre**: "Attention Is All You Need"
- **Auteurs**: Ashish Vaswani, et al.
- **Date**: 2017
- **Importance**: ★★★★★ - Architecture Transformer révolutionnaire
- **Findings clés**:
  - Mécanisme attention pure (sans CNN/RNN)
  - Parallélisation massive possible
  - Base architectures modernes (BERT, GPT)

#### **brown_2020_gpt3_language_models_few_shot.pdf** (6.5 MB) ✅ **NOUVEAU**
- **Titre**: "Language Models are Few-Shot Learners"
- **Auteurs**: Tom B. Brown, et al.
- **Date**: 2020
- **Importance**: ★★★★★ - GPT-3, émergence capacités few-shot
- **Findings clés**:
  - Scaling laws for language models
  - Few-shot learning sans fine-tuning
  - Émergence capacités cognitives complexes

#### **mikolov_2013_word2vec_efficient_estimation.pdf** (223 KB) ✅ **NOUVEAU**
- **Titre**: "Efficient Estimation of Word Representations in Vector Space"
- **Auteurs**: Tomas Mikolov, et al.
- **Date**: 2013
- **Importance**: ★★★★☆ - Word2Vec, représentations distribuées
- **Findings clés**:
  - Skip-gram et CBOW models
  - Captures semantic relationships
  - Foundation modern word embeddings

### 5. Techniques Avancées Compression

#### **entropy_semantic_compression_2509.00503.pdf** (610 KB)
- **Titre**: "Entropy-based Coarse and Compressed Semantic Speech Representation Learning"
- **Date**: Septembre 2025 (très récent)
- **Importance**: Approches entropie pour compression sémantique speech
- **Focus**: Representation learning via entropy optimization

#### **semantic_token_2508.15190.pdf** (3.4 MB)
- **Titre**: "SemToken: Semantic-Aware Tokenization"  
- **Date**: Août 2025
- **Importance**: Tokenisation aware sémantique, alternative aux méthodes standard
- **Focus**: Innovation en segmentation text préservant semantic units

## ANALYSES PRIORITAIRES

### Documents à analyser en détail:
1. **tokens_to_thoughts_2505.17117.pdf** - Framework théorique central
2. **quantum_semantic_framework_2506.10077.pdf** - Validation approche non-classique  
3. **gpt_compression_kolmogorov_2308.06942.pdf** - Benchmarks compression

### Extractions à faire:
- Mathematical frameworks utilisés
- Benchmarks et métriques standards
- Limitations identifiées dans chaque approche
- Convergences avec notre approche 9 dhātu
- Citations et références clés

## PROCHAINES RECHERCHES

### Domaines à approfondir:
1. **Minimal Description Length (MDL)** - Principe et applications sémantiques
2. **Algorithmic Information Theory** - Extensions récentes
3. **Cognitive Semantics** - Modèles computationnels
4. **Universal Grammar** - Primitives syntactico-sémantiques
5. **Compression Bounds** - Limites théoriques sémantiques

## STATISTIQUES CACHE ACTUALISÉES

### Distribution par Domain:
- **Information Theory**: 4 documents (Shannon, Kolmogorov-inspired, Delétang)
- **Semantic Compression**: 3 documents (Tokens-to-Thoughts, Entropy-based, SemToken)
- **Quantum/Non-Classical**: 1 document (Quantum semantic framework)
- **AI/ML Architectures**: 3 documents (Transformer, GPT-3, Word2Vec)
- **Misc**: 1 document (DN06011FU1)

### Total Documents Cache: **12 PDFs**
- **Existants**: 6 documents originaux
- **Nouveaux ajoutés**: 6 documents fondamentaux
- **Taille totale**: ~25 MB

### Documents Prioritaires Manquants:
1. **Wierzbicka, A. (1996)** - "Semantics: Primes and Universals" (NSM foundation)
2. **Lakoff & Johnson (1980)** - "Metaphors We Live By" (Conceptual metaphor)
3. **Fillmore (1976)** - "Frame semantics" (Frame theory)
4. **Kolmogorov (1965)** - "Three approaches to information" (Original complexity)
5. **Zadeh (1965)** - "Fuzzy sets" (Fuzzy logic foundation)

### Prochaines Actions:
- Rechercher sources alternatives pour documents manquants
- Compléter couverture linguistique cognitive
- Ajouter neurosciences développementales (Piaget, Vygotsky)
- Extensions quantum computing pour NLP

### Sources additionnelles à explorer:
- IEEE Transactions on Information Theory (derniers 3 ans)
- Cognitive Science journal (semantic compression)
- ICML/NeurIPS proceedings (representation learning)
- Computational Linguistics (semantic primitives)

---

*Cache créé pour assurer disponibilité permanente des sources critiques et permettre analyse offline approfondie.*
